{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, normalize\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cross_decomposition import PLSRegression, CCA, PLSSVD\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = os.path.join(\n",
    "    \"..\",\n",
    "    \"data_in\",\n",
    "    \"mildiou_source_excels\",\n",
    "    \"\",\n",
    ")\n",
    "\n",
    "mildiou_extracted_csvs_path = os.path.join(\n",
    "    \"..\",\n",
    "    \"data_in\",\n",
    "    \"mildiou_extracted_csvs\",\n",
    "    \"\",\n",
    ")\n",
    "\n",
    "excel_file_list_path = os.path.join(excel_file_path, \"excel_list.txt\")\n",
    "\n",
    "needed_columns = [\"nomphoto\", \"oiv\",\"s\",\"sq\",\"n\",\"fn\",\"tn\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_in_list(required_columns, available_columns):\n",
    "    failures = []\n",
    "    for rc in required_columns:\n",
    "        if rc not in available_columns:\n",
    "            failures.append(rc)\n",
    "\n",
    "    return True if len(failures) == 0 else failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_model(model, df, x_new, column_names, loadings=None, title=\"\", pcx:int=0,pcy:int=1):\n",
    "    pc1_lbl = f\"PC{pcx} ({model.explained_variance_ratio_[pcx] * 100:.2f}%)\"\n",
    "    pc2_lbl = f\"PC2{pcy} ({model.explained_variance_ratio_[pcy] * 100:.2f}%)\"\n",
    "    x = x_new[:, 0]\n",
    "    y = x_new[:, 1]\n",
    "\n",
    "    df[pc1_lbl] = x * (1.0 / (x.max() - x.min()))\n",
    "    df[pc2_lbl] = y * (1.0 / (y.max() - y.min()))\n",
    "\n",
    "    fig = px.scatter(\n",
    "        data_frame=df,\n",
    "        x=pc1_lbl,\n",
    "        y=pc2_lbl,\n",
    "        color=\"oiv_cat\",\n",
    "        title=title,\n",
    "    )    \n",
    "    if loadings is not None:\n",
    "        loadings = loadings / np.amax(loadings)\n",
    "        xc, yc = [], []\n",
    "        for i in range(loadings.shape[0]):\n",
    "            xc.extend([0, loadings[i, pcx], None])\n",
    "            yc.extend([0, loadings[i, pcy], None])\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=xc,\n",
    "                y=yc,\n",
    "                mode=\"lines\",\n",
    "                name=\"Loadings\",\n",
    "                showlegend=False,\n",
    "                line=dict(color=\"black\"),\n",
    "                opacity=0.3,\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=loadings[:, pcx],\n",
    "                y=loadings[:, pcy],\n",
    "                mode=\"text\",\n",
    "                text=column_names,\n",
    "                opacity=0.7,\n",
    "                name=\"Loadings\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance(df_ev):\n",
    "    df_ev = df_ev.assign(cumulative=df_ev[\"exp_var_per\"].cumsum())\n",
    "    ev_fig = go.Figure()\n",
    "    ev_fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_ev[\"pc\"],\n",
    "            y=df_ev[\"exp_var_per\"],\n",
    "            name=\"individual\",\n",
    "        )\n",
    "    )\n",
    "    ev_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_ev[\"pc\"],\n",
    "            y=df_ev[\"cumulative\"],\n",
    "            name=\"cumulative\",\n",
    "        )\n",
    "    )\n",
    "    ev_fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title=\"Explained variance by different principal components\",\n",
    "        xaxis_title=\"Principal component\",\n",
    "        yaxis_title=\"Explained variance in percent\",\n",
    "    )\n",
    "    return ev_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_ellipse(x, y, n_std=1.96, size=100):\n",
    "    \"\"\"\n",
    "    Get the covariance confidence ellipse of *x* and *y*.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "    size : int\n",
    "        Number of points defining the ellipse\n",
    "    Returns\n",
    "    -------\n",
    "    String containing an SVG path for the ellipse\n",
    "\n",
    "    References (H/T)\n",
    "    ----------------\n",
    "    https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html\n",
    "    https://community.plotly.com/t/arc-shape-with-path/7205/5\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    theta = np.linspace(0, 2 * np.pi, size)\n",
    "    ellipse_coords = np.column_stack(\n",
    "        [ell_radius_x * np.cos(theta), ell_radius_y * np.sin(theta)]\n",
    "    )\n",
    "\n",
    "    # Calculating the stdandard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    x_scale = np.sqrt(cov[0, 0]) * n_std\n",
    "    x_mean = np.mean(x)\n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    y_scale = np.sqrt(cov[1, 1]) * n_std\n",
    "    y_mean = np.mean(y)\n",
    "\n",
    "    translation_matrix = np.tile([x_mean, y_mean], (ellipse_coords.shape[0], 1))\n",
    "    rotation_matrix = np.array(\n",
    "        [[np.cos(np.pi / 4), np.sin(np.pi / 4)], [-np.sin(np.pi / 4), np.cos(np.pi / 4)]]\n",
    "    )\n",
    "    scale_matrix = np.array([[x_scale, 0], [0, y_scale]])\n",
    "    ellipse_coords = (\n",
    "        ellipse_coords.dot(rotation_matrix).dot(scale_matrix) + translation_matrix\n",
    "    )\n",
    "\n",
    "    path = f\"M {ellipse_coords[0, 0]}, {ellipse_coords[0, 1]}\"\n",
    "    for k in range(1, len(ellipse_coords)):\n",
    "        path += f\"L{ellipse_coords[k, 0]}, {ellipse_coords[k, 1]}\"\n",
    "    path += \" Z\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.isfile(excel_file_list_path):\n",
    "    with open(excel_file_list_path, \"r\", encoding=\"UTF8\") as f:\n",
    "        files = f.read().split(\"?\")\n",
    "else:\n",
    "    files = [\n",
    "        os.path.join(root, name)\n",
    "        for root, _, files in os.walk(\n",
    "            \"Z:\",\n",
    "            topdown=False,\n",
    "        )\n",
    "        for name in files\n",
    "        if name.endswith(\"_saisie.xlsx\")\n",
    "    ]\n",
    "    with open(excel_file_list_path, \"w+\", encoding=\"UTF8\") as f:\n",
    "        f.write(\"?\".join(files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files):\n",
    "    file_name = os.path.basename(file)\n",
    "    if not file_name.startswith(\"~$\") and not os.path.isfile(\n",
    "        os.path.join(\n",
    "            excel_file_path,\n",
    "            file_name,\n",
    "        )\n",
    "    ):\n",
    "        shutil.copy(src=file, dst=excel_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean excels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List local excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcl_excel_files = [\n",
    "    os.path.join(root, name)\n",
    "    for root, _, files in os.walk(\n",
    "        excel_file_path,\n",
    "        topdown=False,\n",
    "    )\n",
    "    for name in files\n",
    "    if name.endswith(\"_saisie.xlsx\")\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcl_excel_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for 2 particular headers, sheets will be discarded if:\n",
    "- the header is not found\n",
    "- the dataframe is corrupted, ie unable to find images or a column is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df_result = os.path.join(\"..\", \"data_in\", \"excel_extraction.csv\")\n",
    "\n",
    "if os.path.isfile(path_to_df_result):\n",
    "    df_result = pd.read_csv(path_to_df_result)\n",
    "else:\n",
    "    df_result = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"file\",\n",
    "            \"sheet\",\n",
    "            \"outcome\",\n",
    "            \"comment\",\n",
    "            \"csv_file_name\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def add_result(file, sheet, outcome, comment=\"success\", csv_file_name=np.nan):\n",
    "        global df_result\n",
    "        df_result = df_result.append(\n",
    "            {\n",
    "                \"file\": file,\n",
    "                \"sheet\": sheet,\n",
    "                \"outcome\": outcome,\n",
    "                \"comment\": comment,\n",
    "                \"csv_file_name\": csv_file_name,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    def lower_dataframe(df):\n",
    "        try:\n",
    "            df.columns = df.columns.str.lower().str.replace(\" \", \"\")\n",
    "            for c in df.columns:\n",
    "                if c != \"nomphoto\" and df[c].dtype == object:\n",
    "                    df[c] = df[c].str.lower().str.replace(\" \", \"\")\n",
    "        except:\n",
    "            return False\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    for lcl_excel_file in tqdm(lcl_excel_files):\n",
    "        tst_excel_file = pd.ExcelFile(lcl_excel_file)\n",
    "        for sheet_name in tst_excel_file.sheet_names:\n",
    "            df = lower_dataframe(df=tst_excel_file.parse(sheet_name=sheet_name))\n",
    "            if df is False:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=\"Corrupted dataframe\",\n",
    "                )\n",
    "                continue\n",
    "            header_loc = df[df == \"numinc\"].dropna(axis=1, how=\"all\").dropna(how=\"all\")\n",
    "            if header_loc.shape == (0, 0):\n",
    "                header_loc = df[df == \"num\"].dropna(axis=1, how=\"all\").dropna(how=\"all\")\n",
    "                if header_loc.shape == (0, 0):\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=False,\n",
    "                        comment=\"No header\",\n",
    "                    )\n",
    "                    continue\n",
    "            row = header_loc.index.item()\n",
    "            column = header_loc.columns.item()\n",
    "            df = lower_dataframe(\n",
    "                df=tst_excel_file.parse(\n",
    "                    tst_excel_file.sheet_names[0],\n",
    "                    skiprows=row + 1,\n",
    "                )\n",
    "            )\n",
    "            if df is False:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=\"Corrupted dataframe\",\n",
    "                )\n",
    "                continue\n",
    "            if (\n",
    "                res := check_list_in_list(\n",
    "                    required_columns=needed_columns,\n",
    "                    available_columns=df.columns.to_list(),\n",
    "                )\n",
    "            ) is True:\n",
    "                csv_file_name = f\"{Path(lcl_excel_file).stem}_{sheet_name}.csv\"\n",
    "                df = df.assign(\n",
    "                    exp=Path(lcl_excel_file).stem, \n",
    "                    sheet=sheet_name,\n",
    "                ).dropna(\n",
    "                    subset=[\"nomphoto\"]\n",
    "                )\n",
    "                if df.shape[0] > 0:\n",
    "                    df.to_csv(\n",
    "                        os.path.join(mildiou_extracted_csvs_path, csv_file_name),\n",
    "                        index=False,\n",
    "                    )\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=True,\n",
    "                        csv_file_name=csv_file_name,\n",
    "                    )\n",
    "                else:\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=False,\n",
    "                        comment=\"Corrupted dataframe, failed to retrieve photos\",\n",
    "                    )\n",
    "            else:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=f\"Missing columns: {res}\",\n",
    "                )\n",
    "\n",
    "    df_result.to_csv(path_to_df_result, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of sheets parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why sheets were rejected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_result.comment.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debrief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(data=df_result)\n",
    "    + aes(\n",
    "        x=\"comment\",\n",
    "        # color=\"comment\",\n",
    "        fill=\"comment\",\n",
    "    )\n",
    "    + geom_bar()\n",
    "    + theme(\n",
    "        figure_size=(10, 6),\n",
    "        axis_text_x=element_blank(),\n",
    "        # axis.text_x=element_blank()\n",
    "    )\n",
    "    + geom_text(aes(label=after_stat(\"count\")), stat=\"count\", nudge_y=0.125, va=\"bottom\")\n",
    "    + labs(x=\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corrupted dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupted = df_result[\n",
    "    df_result.comment.isin(\n",
    "        [\n",
    "            \"Corrupted dataframe\",\n",
    "            \"Corrupted dataframe, failed to retrieve photos\",\n",
    "        ]\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_corrupted.to_csv(\n",
    "    os.path.join(\"..\", \"data_in\", \"corrupted_excels.csv\"),\n",
    "    index=False,\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "df_corrupted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcl_csv_files = [\n",
    "    os.path.join(mildiou_extracted_csvs_path, filename)\n",
    "    for filename in df_result.csv_file_name.dropna().to_list()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of sheets successfully converted to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcl_csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What columns are common all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = set(pd.read_csv(lcl_csv_files[0]).columns.to_list())\n",
    "columns_occ = {}\n",
    "for filepath in lcl_csv_files:\n",
    "    cu_columns = pd.read_csv(filepath).columns.to_list()\n",
    "    for c in cu_columns:\n",
    "        if c in columns_occ:\n",
    "            columns_occ[c] += 1\n",
    "        else:\n",
    "            columns_occ[c] = 1\n",
    "    common_columns = common_columns.intersection(set(cu_columns))\n",
    "common_columns = list(common_columns)\n",
    "common_columns.sort()\n",
    "common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = sorted(list(columns_occ.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all dataframes and:\n",
    "- Remove rows with unwanted values\n",
    "- Drop unwanted columns\n",
    "- Change column names\n",
    "- Set numerical columns\n",
    "- Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(\n",
    "    [pd.read_csv(filepath)[common_columns] for filepath in lcl_csv_files]\n",
    ")\n",
    "df_final = (\n",
    "    df_final[\n",
    "        (~df_final.oiv.isin([\"md\", \"dm\", \"na\"]))\n",
    "        & (~df_final.n.isin([\"clair\", \"spo\", \"profildéjàobtenu\"]))\n",
    "    ]\n",
    "    .drop([\"n°tubestock\",\"plaque\"], axis=1)\n",
    "    .dropna(subset=[\"oiv\", \"nomphoto\"])\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"exp\": \"experiment\",\n",
    "            \"sheet\": \"sheet\",\n",
    "            \"oiv\": \"oiv\",\n",
    "            \"nomphoto\": \"image_name\",\n",
    "            \"s\": \"sporulation\",\n",
    "            \"fn\": \"surface_necrosee\",\n",
    "            \"n\": \"necrose\",\n",
    "            \"sq\": \"densité_sporulation\",\n",
    "            \"tn\": \"taille_necrose\",\n",
    "        }\n",
    "    )\n",
    "    .sort_values([\"image_name\"])\n",
    "    .assign(\n",
    "        colonne=lambda x: x.colonne.astype(int),\n",
    "        necrose=lambda x: x.necrose.astype('Int64'),\n",
    "        oiv=lambda x: x.oiv.astype(int),\n",
    "        sporulation=lambda x: x.sporulation.astype('Int64'),\n",
    "        surface_necrosee=lambda x: x.surface_necrosee.astype('Int64'),\n",
    "        densité_sporulation=lambda x: x.densité_sporulation.astype('Int64'),\n",
    "        taille_necrose=lambda x: x.taille_necrose.astype('Int64'),\n",
    "        oiv_cat=lambda x: x.oiv.astype(str),\n",
    "    )\n",
    "    .assign(necrose=lambda x: x.necrose.fillna(0))\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.ligne.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.necrose.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distincts_values = {}\n",
    "for col in df_final.select_dtypes(exclude=object).columns:\n",
    "    distincts_values[col] = list(df_final[col].unique())\n",
    "distincts_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Necrosis_ should be only 1 or 0\n",
    "_Sporulation_ should be 1 or 0\n",
    "\n",
    "Most sheets have incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[(df_final.sporulation.isin([0, 1]) | df_final.necrose.isin([0, 1]))][\n",
    "    [\"experiment\", \"sheet\"]\n",
    "].drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[df_final.sporulation.isin([0,1]) & df_final.necrose.isin([0,1])]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are pair values an error ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(data=df_final)\n",
    "    + aes(\n",
    "        x=\"oiv\",\n",
    "        fill=\"oiv_cat\",\n",
    "    )\n",
    "    + geom_bar()\n",
    "    + theme(\n",
    "        figure_size=(10, 6),\n",
    "        axis_text_x=element_blank(),\n",
    "        # axis.text_x=element_blank()\n",
    "    )\n",
    "    + geom_text(aes(label=after_stat(\"count\")), stat=\"count\", nudge_y=0.125, va=\"bottom\")\n",
    "    + labs(x=\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep odd OIVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[df_final.oiv.isin([1, 3, 5, 7, 9])]\n",
    "(\n",
    "    ggplot(data=df_final)\n",
    "    + aes(\n",
    "        x=\"oiv\",\n",
    "        fill=\"oiv_cat\",\n",
    "    )\n",
    "    + geom_bar()\n",
    "    + theme(\n",
    "        figure_size=(10, 6),\n",
    "        axis_text_x=element_blank(),)\n",
    "    )\n",
    "    + geom_text(aes(label=after_stat(\"count\")), stat=\"count\", nudge_y=0.125, va=\"bottom\")\n",
    "    + labs(x=\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = []\n",
    "for c in df_final.columns:\n",
    "    nan_count.append((c, df_final[c].isna().sum()))\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove NaN line row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna(subset=[\"ligne\"]).reset_index(drop=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = (\n",
    "    df_final.sort_values(\n",
    "        [\n",
    "            \"oiv\",\n",
    "            \"experiment\",\n",
    "            \"sheet\",\n",
    "        ]\n",
    "    )\n",
    "    .drop([\"colonne\", \"ligne\"], axis=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona_col = df_all[[\"oiv\",\"necrose\",\"sporulation\", \"oiv_cat\"]].reset_index(drop=True)\n",
    "df_nona_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_all.select_dtypes(exclude=object)\n",
    "df_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_melt = df_all.drop([\"image_name\", \"oiv_cat\"], axis=1).melt(\n",
    "#     id_vars=[\"experiment\", \"sheet\"],\n",
    "#     var_name=\"value\",\n",
    "# )\n",
    "# df_melt.columns = [\"experiment\", \"sheet\", \"value\", \"dump\"]\n",
    "# df_melt = df_melt.drop([\"dump\"], axis=1)\n",
    "# (\n",
    "#     ggplot(data=df_melt)\n",
    "#     + aes(\n",
    "#         x=\"sheet\",\n",
    "#         y=\"value\",\n",
    "#     )\n",
    "#     + geom_bar()\n",
    "#     + theme(\n",
    "#         figure_size=(16, 6),\n",
    "#         # axis_text_x=element_blank(),\n",
    "#         # axis.text_x=element_blank()\n",
    "#     )\n",
    "#     + geom_text(aes(label=after_stat(\"count\")), stat=\"count\", nudge_y=0.125, va=\"bottom\")\n",
    "#     + labs(x=\"\")\n",
    "# )\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(df_num.columns))\n",
    "for i, var in enumerate(df_num.columns):\n",
    "    fig.add_trace(\n",
    "        go.Violin(y=df_num[var], name=var),\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "fig.update_traces(points=\"all\", jitter=0.3).update_layout(\n",
    "    height=1000,\n",
    "    width=1400,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_all.select_dtypes(exclude=object)\n",
    "# df_norm = df_norm / df_norm.max()\n",
    "df_norm[\"oiv_cat\"] = df_all.oiv_cat\n",
    "df_norm[\"oiv\"] = df_all.oiv\n",
    "fig = px.scatter_matrix(\n",
    "    df_norm, \n",
    "    color=\"oiv_cat\",\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    dimensions=df_norm.select_dtypes(np.number).columns\n",
    ")\n",
    "fig.update_yaxes(tickangle=45, tickfont=dict(family='Rockwell', color='crimson', size=14))\n",
    "\n",
    "# df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_all.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=True,\n",
    "    height=800,\n",
    "    width=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df_all.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df_nona.drop([\"oiv\"], axis=1).select_dtypes(exclude=object).dropna()\n",
    "\n",
    "column_names = X.columns.to_list()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "pca_data = PCA()\n",
    "x_new = pca_data.fit_transform(X)\n",
    "\n",
    "x_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_model(\n",
    "    model=pca_data,\n",
    "    df=df_nona.copy(),\n",
    "    x_new=x_new,\n",
    "    column_names=column_names,\n",
    "    # loadings=np.transpose(pca_data.components_[0:2, :]),\n",
    "    title=\"PCA with loadings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_model(\n",
    "    model=pca_data,\n",
    "    df=df_nona.copy(),\n",
    "    x_new=x_new,\n",
    "    column_names=column_names,\n",
    "    loadings=np.transpose(pca_data.components_[0:2, :]),\n",
    "    title=\"PCA with loadings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(\n",
    "    df_ev=pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"pc\": [f\"PC{i}\" for i in range(len(pca_data.explained_variance_ratio_))],\n",
    "            \"exp_var_per\": pca_data.explained_variance_ratio_ * 100,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    x=x_new[:, 0] / x_new[:, 0].max(),\n",
    "    y=x_new[:, 1] / x_new[:, 1].max(),\n",
    "    z=x_new[:, 2] / x_new[:, 2].max(),\n",
    "    color=df_nona.oiv_cat,\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non NaN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_nona_col.drop([\"oiv\"], axis=1).select_dtypes(exclude=object).dropna()\n",
    "\n",
    "column_names = X.columns.to_list()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "pca_data = PCA()\n",
    "x_new = pca_data.fit_transform(X)\n",
    "\n",
    "scatter_model(\n",
    "    model=pca_data,\n",
    "    df=df_nona_col.copy(),\n",
    "    x_new=x_new,\n",
    "    column_names=column_names,\n",
    "    loadings=np.transpose(pca_data.components_[0:2, :]),\n",
    "    title=\"PCA with loadings\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(\n",
    "    df_ev=pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"pc\": [f\"PC{i}\" for i in range(len(pca_data.explained_variance_ratio_))],\n",
    "            \"exp_var_per\": pca_data.explained_variance_ratio_ * 100,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3d = df_nona_col.assign(\n",
    "    sporulation= lambda x: x.sporulation / x.sporulation.max(),\n",
    "    oiv=lambda x: x.oiv / x.oiv.max(),\n",
    "    necrose=lambda x: x.necrose / x.necrose.max(),\n",
    ")\n",
    "\n",
    "px.scatter_3d(\n",
    "    data_frame=df_3d,\n",
    "    x=\"oiv\",\n",
    "    y=\"sporulation\",\n",
    "    z=\"necrose\",\n",
    "    color=\"oiv_cat\",\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"Data 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLs-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df_all.dropna().reset_index(drop=True)\n",
    "X = df_nona.drop([\"oiv\"], axis=1).select_dtypes(exclude=object).dropna()\n",
    "\n",
    "pls_data_all = PLSRegression(n_components=X.shape[1])\n",
    "x_new = pls_data_all.fit(\n",
    "    X, \n",
    "    df_nona.oiv,\n",
    ").transform(X)\n",
    "\n",
    "pls_data_all.score(X, df_nona.oiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=pls_data_all.x_scores_[:, 0] / pls_data_all.x_scores_[:, 0].max(),\n",
    "    y=pls_data_all.x_scores_[:, 1] / pls_data_all.x_scores_[:, 1].max(),\n",
    "    color=df_nona.oiv_cat,\n",
    "    height=800,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "    ),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(target_value, target_name) for target_value, target_name in enumerate(df_nona.oiv.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non NaN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona_col = df_all[[\"oiv\",\"necrose\",\"sporulation\", \"oiv_cat\"]].reset_index(drop=True)\n",
    "df_nona_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_nona_col.drop([\"oiv\", \"oiv_cat\"], axis=1).select_dtypes(exclude=object)\n",
    "\n",
    "pls_data = PLSRegression(n_components=X.shape[1])\n",
    "x_new = pls_data.fit(\n",
    "    X,\n",
    "    df_nona_col.oiv,\n",
    ").transform(X)\n",
    "\n",
    "pls_data.score(X, df_nona_col.oiv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_data.x_scores_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=pls_data.x_scores_[:,0],\n",
    "    y=pls_data.x_scores_[:,1],\n",
    "    color=df_nona_col.oiv_cat,\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    opacity=0.5\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "    ),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
