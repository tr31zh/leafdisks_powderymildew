{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf disk sheat segmentation and disk coordonates detection with SegFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ternausnet.models\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image as IpImage\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"scripts\"))\n",
    "\n",
    "import ld_dataset as ldd\n",
    "import ld_plot as ldp\n",
    "import ld_image as ldi\n",
    "import ld_th_pl_lightning as ldpl\n",
    "import gav_oidium_const as goc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build local images dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "una_images = [i.stem for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "ano_images = [i.stem for i in ldd.train_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "wot_images = [i.stem for i in ldd.wot_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ano_images), len(una_images), len(wot_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_images = list(set(ano_images + una_images + wot_images))\n",
    "len(local_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [\n",
    "    \"experiment\",\n",
    "    \"rep\",\n",
    "    \"image_name\",\n",
    "    \"ligne\",\n",
    "    \"colonne\",\n",
    "    \"oiv\",\n",
    "    \"sporulation\",\n",
    "    \"densite_sporulation\",\n",
    "    \"necrose\",\n",
    "    \"surface_necrosee\",\n",
    "    \"taille_necrose\",\n",
    "]\n",
    "\n",
    "df = (\n",
    "    pd.read_csv(os.path.join(\"..\", \"data_in\", \"raw_merged.csv\"), sep=\";\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.image_name.str.lower().replace(\"_\", \"-\").isin([l.lower().replace(\"_\", \"-\") for l in local_images])]\n",
    "\n",
    "df.sort_values(\"image_name\").to_csv(\n",
    "    os.path.join(\"..\", \"data_in\", \"local_raw_merged.csv\"),\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldpl.g_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check image duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "una_images = [str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "ano_images = [str(i) for i in ldd.train_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "\n",
    "len(una_images), len(ano_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check anotated images in not annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img for img in ano_images if img in una_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check not anotated images in annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img for img in una_images if img in ano_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images = ldd.build_items_dataframe(\n",
    "    images_folder=ldd.train_images_folder,\n",
    "    masks_folder=ldd.train_masks_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd.check_items_consistency(\n",
    "    images_folder=ldd.train_images_folder,\n",
    "    masks_folder=ldd.train_masks_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train_images, test_size=0.3, stratify=df_train_images[\"year\"])\n",
    "test, val = train_test_split(test, test_size=0.5, stratify=test[\"year\"])\n",
    "\n",
    "print(len(train), len(test), len(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set_counts = pd.DataFrame(\n",
    "    index=df_train_images.year.sort_values().value_counts(sort=False).index,\n",
    "    data={\n",
    "        \"train\": train.year.sort_values().value_counts(sort=False).values,\n",
    "        \"val\": val.year.sort_values().value_counts(sort=False).values,\n",
    "        \"test\": test.year.sort_values().value_counts(sort=False).values,\n",
    "    },\n",
    ")\n",
    "df_set_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set_counts.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(train.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(val.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(test.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 32 * 21\n",
    "img_height = 32 * 14\n",
    "\n",
    "assert(img_width / img_height == 1.5)\n",
    "\n",
    "alb_resizer = [A.Resize(height=img_height, width=img_width)]\n",
    "\n",
    "train_transformers_list = alb_resizer + [\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.RandomGamma(p=0.33),\n",
    "    A.CLAHE(p=0.33),\n",
    "]\n",
    "\n",
    "to_tensor = [\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "transformer = A.Compose(train_transformers_list)\n",
    "\n",
    "image, mask = ldd.open_image_and_mask(\n",
    "    0, df_train_images.sample(n=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "ldp.visualize_augmentations(\n",
    "    image=image,\n",
    "    mask=mask,\n",
    "    augmentations=[transformer(image=image, mask=mask) for _ in range(5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_path(image_name: str, df: pd.DataFrame):\n",
    "    try:\n",
    "        mask_path = df[df.image_name == image_name].mask_path.to_list()[0]\n",
    "    except:\n",
    "        return None\n",
    "    else:\n",
    "        return mask_path\n",
    "\n",
    "\n",
    "def get_image_path(image_name, df: pd.DataFrame):\n",
    "    try:\n",
    "        image_path = df[df.image_name == image_name].image_path.to_list()[0]\n",
    "    except:\n",
    "        return None\n",
    "    else:\n",
    "        return image_path\n",
    "\n",
    "\n",
    "def get_image_name_at(idx: int, df: pd.DataFrame) -> str:\n",
    "    return df.image_name.to_list()[0]\n",
    "\n",
    "\n",
    "def open_mask(path, df: pd.DataFrame = None):\n",
    "    mask_path = path if df is None else get_mask_path(image_name=path, df=df)\n",
    "    if mask_path is not None:\n",
    "        # mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        # mask[mask == 0.0] = 0.0\n",
    "        # mask[mask != 0.0] = 1.0\n",
    "        # return mask\n",
    "        return Image.open(mask_path)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def open_image(path, df: pd.DataFrame = None):\n",
    "    return Image.open(path if df is None else get_image_path(image_name=path, df=df))\n",
    "    # return cv2.cvtColor(\n",
    "    #     cv2.imread(path if df is None else get_image_path(image_name=path, df=df)),\n",
    "    #     cv2.COLOR_BGR2RGB,\n",
    "    # )\n",
    "\n",
    "\n",
    "def open_image_and_mask(key, df: pd.DataFrame):\n",
    "    if isinstance(key, str):\n",
    "        return open_image(key, df), open_mask(key, df)\n",
    "    if isinstance(key, int):\n",
    "        filename = get_image_name_at(key, df)\n",
    "        return open_image(filename, df), open_mask(filename, df)\n",
    "    return None, None\n",
    "\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df_img, feature_extractor, augmenter):\n",
    "        self.df_img = df_img\n",
    "        self.augmenter = augmenter\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_img.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, segmentation_map = open_image_and_mask(key=idx, df=self.df_img)\n",
    "        # if self.augmenter is not None:\n",
    "        #     augmented = self.augmenter(image=image, mask=segmentation_map)\n",
    "        #     image = augmented[\"image\"]\n",
    "        #     segmentation_map = augmented[\"mask\"]\n",
    "\n",
    "        # image = np.moveaxis(image, 2, 0)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.feature_extractor(\n",
    "            image, segmentation_map, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        for k, v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_()  # remove batch dimension\n",
    "\n",
    "            return encoded_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor(reduce_labels=True, do_resize=False)\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(\n",
    "    df_img=train,\n",
    "    feature_extractor=feature_extractor,\n",
    "    augmenter=A.Compose(train_transformers_list),\n",
    ")\n",
    "valid_dataset = SemanticSegmentationDataset(\n",
    "    df_img=val,\n",
    "    feature_extractor=feature_extractor,\n",
    "    augmenter=A.Compose(alb_resizer),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"].squeeze().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in batch.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (batch[\"labels\"] != 255)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"][mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "import json\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "\n",
    "# load id2label mapping from a JSON on the hub\n",
    "\n",
    "id2label = {0:\"background\", 255:\"leaf disk\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# move model to GPU\n",
    "device = \"cpu\"#ldpl.g_device\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].type(torch.float).to(device)\n",
    "        labels = batch[\"labels\"].type(torch.float).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate\n",
    "        with torch.no_grad():\n",
    "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "          predicted = upsampled_logits.argmax(dim=1)\n",
    "          \n",
    "          # note that the metric expects predictions + labels as numpy arrays\n",
    "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "        # let's print loss and metrics every 100 batches\n",
    "        if idx % 100 == 0:\n",
    "          metrics = metric._compute(num_labels=len(id2label), \n",
    "                                   ignore_index=255,\n",
    "                                   reduce_labels=False, # we've already reduced the labels before)\n",
    "          )\n",
    "\n",
    "          print(\"Loss:\", loss.item())\n",
    "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 32 * 21\n",
    "img_height = 32 * 14\n",
    "\n",
    "assert(img_width / img_height == 1.5)\n",
    "\n",
    "alb_resizer = [A.Resize(height=img_height, width=img_width)]\n",
    "\n",
    "train_transformers_list = alb_resizer + [\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.RandomGamma(p=0.33),\n",
    "    A.CLAHE(p=0.33),\n",
    "]\n",
    "\n",
    "to_tensor = [\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "model = ldpl.LeafDiskSegmentation(\n",
    "    batch_size=16,\n",
    "    selected_device=ldpl.g_device,\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=400,\n",
    "    num_workers=0,\n",
    "    train_augmentations=A.Compose(train_transformers_list + to_tensor),\n",
    "    train_data=train,\n",
    "    val_augmentations=A.Compose(alb_resizer + to_tensor),\n",
    "    val_data=val,\n",
    "    accumulate_grad_batches=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=model.max_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15, min_delta=0.0005),\n",
    "        DeviceStatsMonitor(),\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=3,\n",
    "            monitor=\"val_loss\",\n",
    "            auto_insert_metric_name=True,\n",
    "            filename=\"{epoch}-{step}-{train_loss:.5}-{val_loss:.5f}\",\n",
    "        ),\n",
    "    ],\n",
    "    accumulate_grad_batches=model.accumulate_grad_batches,\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_overview = ldpl.update_overviews(test).sort_values([\"test_loss\"])\n",
    "version_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ldpl.LeafDiskSegmentationPredictor()\n",
    "\n",
    "op_df_versions_overview = widgets.Output()\n",
    "with op_df_versions_overview:\n",
    "    display(\n",
    "        version_overview.drop([\"checkpoint_fileName\"], axis=1).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "cb_select_version = widgets.Dropdown(\n",
    "    options=[\"-1 | Select version\"]\n",
    "    + [\n",
    "        f\"{i} | {fn}\"\n",
    "        for i, fn in enumerate(version_overview.checkpoint_fileName.to_list())\n",
    "    ],\n",
    "    description=\"Select version\",\n",
    ")\n",
    "\n",
    "cb_image = widgets.Dropdown(\n",
    "    options=sorted([str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]),\n",
    "    description=\"Select an image:\",\n",
    ")\n",
    "\n",
    "cb_ancillary_image = widgets.Dropdown(\n",
    "    options=[\"Raw mask\", \"Cleaned mask\", \"Probabilities\"],\n",
    "    description=\"Ancillary image\",\n",
    ")\n",
    "\n",
    "src_image = widgets.Output()\n",
    "ancillary_image = widgets.Output()\n",
    "pred_image = widgets.Output()\n",
    "\n",
    "\n",
    "def update_images(image_path, anc_mode):\n",
    "    if predictor.model is None:\n",
    "        src_image.clear_output()\n",
    "        ancillary_image.clear_output()\n",
    "        pred_image.clear_output()\n",
    "        return\n",
    "\n",
    "    image = ldd.open_image(image_path)\n",
    "\n",
    "    predicted_mask = predictor.predict_image(image_path)\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    anc_img = (\n",
    "        predicted_mask\n",
    "        if anc_mode == \"Raw mask\"\n",
    "        else clean_mask\n",
    "        if anc_mode == \"Cleaned mask\"\n",
    "        else predictor.predict_image(image_path, return_probabilities=True)\n",
    "        if anc_mode == \"Probabilities\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    src_image.clear_output()\n",
    "    with src_image:\n",
    "        ldp.visualize_image(\n",
    "            image=image,\n",
    "            title=Path(image_path).stem,\n",
    "        )\n",
    "\n",
    "    ancillary_image.clear_output()\n",
    "    with ancillary_image:\n",
    "        plt.imshow(anc_img, cmap=plt.cm.RdPu)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    pred_image.clear_output()\n",
    "    with pred_image:\n",
    "        ldp.visualize_image(\n",
    "            ldi.print_contours_indexes(\n",
    "                clean_mask,\n",
    "                contours,\n",
    "                canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "            ),\n",
    "            figsize=(12, 8),\n",
    "        )\n",
    "\n",
    "\n",
    "def on_image_changed(change):\n",
    "    update_images(change.new, cb_ancillary_image.value)\n",
    "\n",
    "\n",
    "def on_anc_mode_changed(change):\n",
    "    update_images(cb_image.value, change.new)\n",
    "\n",
    "\n",
    "def on_version_changed(change):\n",
    "    idx, filename = str(change.new).replace(\" \", \"\").split(\"|\")\n",
    "    if int(idx) >= 0:\n",
    "        global predictor\n",
    "        predictor.model = ldpl.LeafDiskSegmentation.load_from_checkpoint(filename)\n",
    "        update_images(cb_image.value, cb_ancillary_image.value)\n",
    "    else:\n",
    "        predictor.model = None\n",
    "\n",
    "\n",
    "cb_select_version.observe(on_version_changed, names=\"value\")\n",
    "cb_image.observe(on_image_changed, names=\"value\")\n",
    "cb_ancillary_image.observe(on_anc_mode_changed, names=\"value\")\n",
    "\n",
    "display(\n",
    "    VBox(\n",
    "        [\n",
    "            HBox([op_df_versions_overview, cb_select_version]),\n",
    "            HBox([cb_image, cb_ancillary_image]),\n",
    "            HBox([VBox([src_image, ancillary_image]), pred_image]),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(cb_select_version.value).replace(\" \", \"\").split(\"|\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all available not used images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out_fld = ldd.root_folder.parent.parent.joinpath(\"data_out\", \"predictions\", datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "data_out_fld.mkdir(parents=True, exist_ok=True)\n",
    "data_out_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(ldd.una_images_folder.glob(\"*\")):\n",
    "    if i.stem.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    image = ldd.open_image(str(i))\n",
    "    predicted_mask = predictor.predict_image(str(i))\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    image = Image.fromarray(\n",
    "        ldi.print_contours_indexes(\n",
    "            clean_mask,\n",
    "            contours,\n",
    "            canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "        )\n",
    "    )\n",
    "    image.save(str(data_out_fld.joinpath(i.name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve isolated leaf disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_src_image = widgets.Dropdown(\n",
    "    options=sorted(\n",
    "        [str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "    ),\n",
    "    description=\"Select a leaf dislk:\",\n",
    ")\n",
    "cb_row = widgets.Dropdown(options=[\"A\", \"B\", \"C\"])\n",
    "cb_col = widgets.Dropdown(options=[1, 2, 3, 4])\n",
    "\n",
    "op_full_plate = widgets.Output()\n",
    "op_leafdisk = widgets.Output()\n",
    "op_leafdisk_no_bck = widgets.Output()\n",
    "\n",
    "\n",
    "def show_leaf_disk(image_path, row, col):\n",
    "    image = ldd.open_image(image_path)\n",
    "\n",
    "    predicted_mask = predictor.predict_image(image_path)\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    op_full_plate.clear_output()\n",
    "    op_leafdisk.clear_output()\n",
    "    op_leafdisk_no_bck.clear_output()\n",
    "\n",
    "    with op_full_plate:\n",
    "        ldp.visualize_image(\n",
    "            ldi.print_single_contour(\n",
    "                clean_mask,\n",
    "                contours,\n",
    "                row=row,\n",
    "                col=col,\n",
    "                canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "            ),\n",
    "            figsize=(12, 8),\n",
    "        )\n",
    "\n",
    "    with op_leafdisk:\n",
    "        ldp.visualize_image(ldi.get_leaf_disk(image.copy(), contours, row, col))\n",
    "\n",
    "    with op_leafdisk_no_bck:\n",
    "        ldp.visualize_image(\n",
    "            ldi.get_leaf_disk(\n",
    "                image.copy(),\n",
    "                contours,\n",
    "                row,\n",
    "                col,\n",
    "                mask=clean_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def on_ld_image_changed(change):\n",
    "    show_leaf_disk(change.new, cb_row.value, cb_col.value)\n",
    "\n",
    "\n",
    "def on_ld_row_changed(change):\n",
    "    show_leaf_disk(cb_src_image.value, change.new, cb_col.value)\n",
    "\n",
    "\n",
    "def on_ld_col_changed(change):\n",
    "    show_leaf_disk(cb_src_image.value, cb_row.value, change.new)\n",
    "\n",
    "\n",
    "cb_src_image.observe(on_ld_col_changed, names=\"value\")\n",
    "cb_row.observe(on_ld_row_changed, names=\"value\")\n",
    "cb_col.observe(on_ld_col_changed, names=\"value\")\n",
    "\n",
    "display(\n",
    "    VBox(\n",
    "        [\n",
    "            HBox([cb_src_image, cb_row, cb_col]),\n",
    "            HBox([op_full_plate]),\n",
    "            HBox([op_leafdisk_no_bck, op_leafdisk]),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
