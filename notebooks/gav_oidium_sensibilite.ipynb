{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will:\n",
    "- Retrieve all available Excel files\n",
    "- Translate them to CSV and merge them\n",
    "- Build models to asses the possibility of predicting OIV from various visual variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need:\n",
    "- Base python libraries for file management\n",
    "- tqdm for progress tracking\n",
    "- Pandas and Numpy for the dataframes\n",
    "- SkLearn for statistics\n",
    "- Plotly for ... plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, normalize\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cross_decomposition import PLSRegression, CCA, PLSSVD\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.float_format = '{:4,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consts for paths and the columns needed in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = os.path.join(\"..\", \"data_in\", \"oidium_source_excels\", \"\")\n",
    "oidium_extracted_csvs_path = os.path.join(\"..\", \"data_in\", \"oidium_extracted_csvs\", \"\")\n",
    "excel_file_list_path = os.path.join(excel_file_path, \"excel_list.txt\")\n",
    "\n",
    "needed_columns = [\"nomphoto\", \"oiv\", \"s\", \"sq\", \"n\", \"fn\", \"tn\", \"ligne\", \"colonne\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataframe has at least the needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_in_list(required_columns, available_columns):\n",
    "    failures = []\n",
    "    for rc in required_columns:\n",
    "        if rc not in available_columns:\n",
    "            failures.append(rc)\n",
    "\n",
    "    return True if len(failures) == 0 else failures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance(df_ev):\n",
    "    df_ev = df_ev.assign(cumulative=df_ev[\"exp_var_per\"].cumsum())\n",
    "    ev_fig = go.Figure()\n",
    "    ev_fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_ev[\"pc\"],\n",
    "            y=df_ev[\"exp_var_per\"],\n",
    "            name=\"individual\",\n",
    "        )\n",
    "    )\n",
    "    ev_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_ev[\"pc\"],\n",
    "            y=df_ev[\"cumulative\"],\n",
    "            name=\"cumulative\",\n",
    "        )\n",
    "    )\n",
    "    ev_fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title=\"Explained variance by different principal components\",\n",
    "        xaxis_title=\"Principal component\",\n",
    "        yaxis_title=\"Explained variance in percent\",\n",
    "    )\n",
    "    return ev_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an histogram of the variables needed for the OIV so inconsistencies can be detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inconsistencies(df, sort_values: bool = True):\n",
    "    columns = [\n",
    "        [\"sporulation\", \"densite_sporulation\", \"\"],\n",
    "        [\"necrose\", \"surface_necrosee\", \"taille_necrose\"],\n",
    "        [\"ligne\", \"colonne\", \"oiv\"],\n",
    "    ]\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=3, subplot_titles=np.array(columns).flatten())\n",
    "\n",
    "    for idl, l in enumerate(columns):\n",
    "        for idc, c in enumerate(l):\n",
    "            if not c:\n",
    "                continue\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df[c].sort_values().astype(str) if sort_values is True else df[c].astype(str),\n",
    "                    texttemplate=\"%{y}\",\n",
    "                    textfont_size=20,\n",
    "                    name=c,\n",
    "                ),\n",
    "                row=idl + 1,\n",
    "                col=idc + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        width=1400,\n",
    "        title=\"Dataframe consistency check\",\n",
    "        xaxis_title=\"Value\",\n",
    "        yaxis_title=\"Count\",\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate oiv_cat from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oiv_cat(df):\n",
    "    return df.oiv.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all related file's path in the distant server\n",
    "\n",
    "- Files containing DM for domny mildew, ie mildiou, are selected for OIV analysis\n",
    "- Files containing PM for powdery mildew, ie o√Ødium, are discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(excel_file_list_path):\n",
    "    with open(excel_file_list_path, \"r\", encoding=\"UTF8\") as f:\n",
    "        files = f.read().split(\"?\")\n",
    "else:\n",
    "    files = [\n",
    "        os.path.join(root, name)\n",
    "        for root, _, files in tqdm(os.walk(\"Z:\",topdown=False))\n",
    "        for name in files\n",
    "        if name.endswith(\"_saisie.xlsx\") and \"DM\" in name\n",
    "    ]\n",
    "    with open(excel_file_list_path, \"w+\", encoding=\"UTF8\") as f:\n",
    "        f.write(\"?\".join(files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of files selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filenames starting with \"~\" are system related and will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files):\n",
    "    file_name = os.path.basename(file)\n",
    "    if not file_name.startswith(\"~$\") and not os.path.isfile(\n",
    "        os.path.join(\n",
    "            excel_file_path,\n",
    "            file_name,\n",
    "        )\n",
    "    ):\n",
    "        shutil.copy(src=file, dst=excel_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean excels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List local excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcl_excel_files = [\n",
    "    os.path.join(root, name)\n",
    "    for root, _, files in os.walk(\n",
    "        excel_file_path,\n",
    "        topdown=False,\n",
    "    )\n",
    "    for name in files\n",
    "    if name.endswith(\"_saisie.xlsx\")\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcl_excel_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up a weird thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheet \"\" in the file \"\" renders a large amount of NaN values that are not found later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weird = pd.ExcelFile(os.path.join(\"..\", \"data_in\", \"Exp19DM04_inoc2_saisie.xlsx\")).parse(sheet_name=\"fichier total\", skiprows=8)\n",
    "df_weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weird.N.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for 2 particular headers, sheets will be discarded if:\n",
    "- the header is not found\n",
    "- the dataframe is corrupted, ie unable to find images or a column is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df_result = os.path.join(\"..\", \"data_in\", \"excel_extraction.csv\")\n",
    "\n",
    "if os.path.isfile(path_to_df_result):\n",
    "    df_result = pd.read_csv(path_to_df_result)\n",
    "else:\n",
    "    df_result = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"file\",\n",
    "            \"sheet\",\n",
    "            \"outcome\",\n",
    "            \"comment\",\n",
    "            \"csv_file_name\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def add_result(\n",
    "        file,\n",
    "        sheet,\n",
    "        outcome,\n",
    "        comment=\"success\",\n",
    "        csv_file_name=np.nan,\n",
    "    ):\n",
    "        global df_result\n",
    "        df_result = df_result.append(\n",
    "            {\n",
    "                \"file\": file,\n",
    "                \"sheet\": sheet,\n",
    "                \"outcome\": outcome,\n",
    "                \"comment\": comment,\n",
    "                \"csv_file_name\": csv_file_name,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    def lower_dataframe(df):\n",
    "        try:\n",
    "            df.columns = df.columns.str.lower().str.replace(\" \", \"\")\n",
    "            for c in df.columns:\n",
    "                if c != \"nomphoto\" and df[c].dtype == object:\n",
    "                    df[c] = df[c].str.lower().str.replace(\" \", \"\")\n",
    "        except:\n",
    "            return False\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    for lcl_excel_file in tqdm(lcl_excel_files):\n",
    "        tst_excel_file = pd.ExcelFile(lcl_excel_file)\n",
    "        for sheet_name in tst_excel_file.sheet_names:\n",
    "            df = lower_dataframe(df=tst_excel_file.parse(sheet_name=sheet_name))\n",
    "            if df is False:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=\"Corrupted dataframe\",\n",
    "                )\n",
    "                continue\n",
    "            header_loc = df[df == \"numinc\"].dropna(axis=1, how=\"all\").dropna(how=\"all\")\n",
    "            if header_loc.shape == (0, 0):\n",
    "                header_loc = df[df == \"num\"].dropna(axis=1, how=\"all\").dropna(how=\"all\")\n",
    "                if header_loc.shape == (0, 0):\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=False,\n",
    "                        comment=\"No header\",\n",
    "                    )\n",
    "                    continue\n",
    "            column = header_loc.columns.item()\n",
    "            df = lower_dataframe(\n",
    "                df=tst_excel_file.parse(\n",
    "                    sheet_name,\n",
    "                    skiprows=header_loc.index.item() + 1,\n",
    "                )\n",
    "            )\n",
    "            if df is False:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=\"Corrupted dataframe\",\n",
    "                )\n",
    "                continue\n",
    "            if (\n",
    "                res := check_list_in_list(\n",
    "                    required_columns=needed_columns,\n",
    "                    available_columns=df.columns.to_list(),\n",
    "                )\n",
    "            ) is True:\n",
    "                csv_file_name = f\"{Path(lcl_excel_file).stem}_{sheet_name}.csv\"\n",
    "                df = df.assign(\n",
    "                    exp=Path(lcl_excel_file).stem,\n",
    "                    sheet=sheet_name,\n",
    "                ).dropna(subset=[\"nomphoto\"])\n",
    "                if df.shape[0] > 0:\n",
    "                    df.to_csv(\n",
    "                        os.path.join(oidium_extracted_csvs_path, csv_file_name),\n",
    "                        index=False,\n",
    "                    )\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=True,\n",
    "                        csv_file_name=csv_file_name,\n",
    "                    )\n",
    "                else:\n",
    "                    add_result(\n",
    "                        file=os.path.basename(lcl_excel_file),\n",
    "                        sheet=sheet_name,\n",
    "                        outcome=False,\n",
    "                        comment=\"Corrupted dataframe, failed to retrieve photos\",\n",
    "                    )\n",
    "            else:\n",
    "                add_result(\n",
    "                    file=os.path.basename(lcl_excel_file),\n",
    "                    sheet=sheet_name,\n",
    "                    outcome=False,\n",
    "                    comment=f\"Missing columns: {res}\",\n",
    "                )\n",
    "\n",
    "    df_result.to_csv(path_to_df_result, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of sheets parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why sheets were rejected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    data_frame=df_result.sort_values([\"comment\"]),\n",
    "    x=\"comment\",\n",
    "    color=\"comment\",\n",
    "    width=1400,\n",
    "    height=800,\n",
    "    text_auto=True,\n",
    ").update_layout(\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corrupted dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the cause af the corruption is \"Corrupted dataframe\", the files are info files without data\n",
    "- When the cause af the corruption is \"Corrupted dataframe, failed to retrieve photos\" a formula has an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupted = df_result[\n",
    "    df_result.comment.isin(\n",
    "        [\n",
    "            \"Corrupted dataframe\",\n",
    "            \"Corrupted dataframe, failed to retrieve photos\",\n",
    "        ]\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_corrupted.to_csv(\n",
    "    os.path.join(\"..\", \"data_in\", \"corrupted_excels.csv\"),\n",
    "    index=False,\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "df_corrupted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcl_csv_files = [\n",
    "    os.path.join(oidium_extracted_csvs_path, filename)\n",
    "    for filename in df_result.csv_file_name.dropna().to_list()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of sheets successfully converted to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lcl_csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About The Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What columns are common all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = set(pd.read_csv(lcl_csv_files[0]).columns.to_list())\n",
    "columns_occ = {}\n",
    "for filepath in lcl_csv_files:\n",
    "    cu_columns = pd.read_csv(filepath).columns.to_list()\n",
    "    for c in cu_columns:\n",
    "        if c in columns_occ:\n",
    "            columns_occ[c] += 1\n",
    "        else:\n",
    "            columns_occ[c] = 1\n",
    "    common_columns = common_columns.intersection(set(cu_columns))\n",
    "common_columns = list(common_columns)\n",
    "common_columns.sort()\n",
    "common_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all the columns found in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = sorted(list(columns_occ.keys()))\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_merged = (\n",
    "    pd.concat([pd.read_csv(filepath)[common_columns] for filepath in lcl_csv_files])\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"exp\": \"experiment\",\n",
    "            \"sheet\": \"sheet\",\n",
    "            \"oiv\": \"oiv\",\n",
    "            \"nomphoto\": \"image_name\",\n",
    "            \"s\": \"sporulation\",\n",
    "            \"fn\": \"surface_necrosee\",\n",
    "            \"n\": \"necrose\",\n",
    "            \"sq\": \"densite_sporulation\",\n",
    "            \"tn\": \"taille_necrose\",\n",
    "        }\n",
    "    )\n",
    "    .drop([\"n¬∞tubestock\", \"plaque\"], axis=1)\n",
    ")\n",
    "df_raw_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check weird sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_merged[\n",
    "    (df_raw_merged.experiment == \"Exp19DM04_inoc2_saisie\")\n",
    "    & (df_raw_merged.sheet == \"fichier total\")\n",
    "    & ~(df_raw_merged.sporulation.isna())\n",
    "    & ~(df_raw_merged.necrose.isna())\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different observations are here at the beginning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_merged.drop(\n",
    "    [\"colonne\", \"experiment\", \"ligne\", \"image_name\", \"sheet\"], axis=1\n",
    ").drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove observations with wrong values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inconsistencies(df_raw_merged, sort_values=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLean dataframe\n",
    "\n",
    "- _Sporulation_ must be 1 ou 0\n",
    "- _densit√© sporulation_a number and not 0\n",
    "- _Necrosis_ must be 1 ou 0\n",
    "- _Surface_necrosee must be an integer or NaN\n",
    "- _taille_necrose_ must be an integer or NaN\n",
    "- _ligne_ must not be NaN\n",
    "- _colonne_ must not be NaN\n",
    "- _OIV_ must be an odd integer\n",
    "\n",
    "Sheets that have incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_numbers = [n for n in [1,3,5,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_merged = df_raw_merged[\n",
    "    (\n",
    "        df_raw_merged.sporulation.isin([0,1])\n",
    "        & (df_raw_merged.densite_sporulation.isin(odd_numbers) | df_raw_merged.densite_sporulation.isna())\n",
    "        & df_raw_merged.necrose.isin([0,1])\n",
    "        & df_raw_merged.ligne.notna()\n",
    "        & df_raw_merged.oiv.isin(odd_numbers)\n",
    "        & (df_raw_merged.taille_necrose.isin(odd_numbers) | df_raw_merged.taille_necrose.isna())\n",
    "        & (df_raw_merged.surface_necrosee.isin(odd_numbers) | df_raw_merged.surface_necrosee.isna())\n",
    "    )\n",
    "]\n",
    "plot_inconsistencies(df_clean_merged, sort_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataframe with all Excels and sheets that contain corrupted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistent = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_raw_merged[~df_raw_merged.sporulation.isin([0, 1])].assign(\n",
    "                because=\"sporulation\"\n",
    "            ),\n",
    "            df_raw_merged[\n",
    "                ~(\n",
    "                    df_raw_merged.densite_sporulation.isin(odd_numbers)\n",
    "                    | df_raw_merged.densite_sporulation.isna()\n",
    "                )\n",
    "            ].assign(because=\"densite_sporulation\"),\n",
    "            df_raw_merged[~df_raw_merged.necrose.isin([0, 1])].assign(because=\"necrose\"),\n",
    "            df_raw_merged[~df_raw_merged.ligne.notna()].assign(because=\"ligne\"),\n",
    "            df_raw_merged[\n",
    "                ~(\n",
    "                    df_raw_merged.taille_necrose.isin(odd_numbers)\n",
    "                    | df_raw_merged.taille_necrose.isna()\n",
    "                )\n",
    "            ].assign(because=\"taille_necrose\"),\n",
    "            df_raw_merged[\n",
    "                ~(\n",
    "                    df_raw_merged.surface_necrosee.isin(odd_numbers)\n",
    "                    | df_raw_merged.surface_necrosee.isna()\n",
    "                )\n",
    "            ].assign(because=\"surface_necrosee\"),\n",
    "            df_raw_merged[~df_raw_merged.oiv.isin(odd_numbers)].assign(because=\"oiv\"),\n",
    "        ]\n",
    "    )[[\"experiment\", \"sheet\", \"because\"]]\n",
    "    .sort_values([\"experiment\", \"sheet\", \"because\"])\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_inconsistent = (\n",
    "    df_inconsistent.assign(\n",
    "        sporulation=np.where(df_inconsistent.because == \"sporulation\", 1, 0),\n",
    "        densite_sporulation=np.where(\n",
    "            df_inconsistent.because == \"densite_sporulation\", 1, 0\n",
    "        ),\n",
    "        necrose=np.where(df_inconsistent.because == \"necrose\", 1, 0),\n",
    "        ligne=np.where(df_inconsistent.because == \"ligne\", 1, 0),\n",
    "        taille_necrose=np.where(df_inconsistent.because == \"taille_necrose\", 1, 0),\n",
    "        surface_necrosee=np.where(df_inconsistent.because == \"surface_necrosee\", 1, 0),\n",
    "        oiv=np.where(df_inconsistent.because == \"oiv\", 1, 0),\n",
    "    )\n",
    "    .drop([\"because\"], axis=1)\n",
    "    .groupby([\"experiment\", \"sheet\"])\n",
    "    .agg(\"sum\")\n",
    "    .reset_index(drop=False)\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "df_inconsistent.to_csv(\n",
    "    os.path.join(\"..\", \"data_in\", \"inconsistent_excels.csv\"),\n",
    "    index=False,\n",
    "    sep=\";\",\n",
    ")\n",
    "\n",
    "df_inconsistent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all dataframes and:\n",
    "- Remove rows with unwanted values\n",
    "- Drop unwanted columns\n",
    "- Change column names\n",
    "- Set numerical columns\n",
    "- Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = (\n",
    "    df_clean_merged.sort_values([\"image_name\"])\n",
    "    .assign(\n",
    "        colonne=lambda x: x.colonne.astype(\"Int64\"),\n",
    "        necrose=lambda x: x.necrose.astype(\"Int64\"),\n",
    "        oiv=lambda x: x.oiv.astype(\"Int64\"),\n",
    "        sporulation=lambda x: x.sporulation.astype(\"Int64\"),\n",
    "        surface_necrosee=lambda x: x.surface_necrosee.astype(\"Int64\"),\n",
    "        densite_sporulation=lambda x: x.densite_sporulation.astype(\"Int64\"),\n",
    "        taille_necrose=lambda x: x.taille_necrose.astype(\"Int64\"),\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "cols = df_merged.columns.to_list()\n",
    "cols = [cols[1] ,cols[8] ,cols[5] ,cols[3] ,cols[0] ,cols[4] ,cols[2] ,cols[7] ,cols[9] ,cols[10] , cols[6] ]\n",
    "df_merged = df_merged[cols]\n",
    "\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inconsistencies(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more NaN values for _taille_necrose_ and _surface_necrose_ than there are plants with necrosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    x=df_merged.oiv.sort_values().astype(str),\n",
    "    color=df_merged.oiv.sort_values().astype(str),\n",
    "    text_auto=True,\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = []\n",
    "for c in df_merged.columns:\n",
    "    nan_count.append((c, df_merged[c].isna().sum()))\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = (\n",
    "    df_merged.drop([\"colonne\"], axis=1)\n",
    "    .select_dtypes(exclude=object)\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=len(df_num.columns))\n",
    "for i, var in enumerate(df_num.columns):\n",
    "    fig.add_trace(\n",
    "        go.Violin(y=df_num[var], name=var),\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    df_num,\n",
    "    color=get_oiv_cat(df_num),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    dimensions=df_num.select_dtypes(np.number).columns,\n",
    ")\n",
    "fig.update_yaxes(tickangle=45, tickfont=dict(family=\"Rockwell\", color=\"crimson\", size=14))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_num.drop_duplicates().corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=True,\n",
    "    height=600,\n",
    "    width=800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataframe for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_num.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "y = X.oiv\n",
    "X = X.drop([\"oiv\"], axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = PCA()\n",
    "x_new = pca_data.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    x=x_new[:, 0] / x_new[:, 0].max(),\n",
    "    y=x_new[:, 1] / x_new[:, 1].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(\n",
    "    df_ev=pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"pc\": [f\"PC{i}\" for i in range(len(pca_data.explained_variance_ratio_))],\n",
    "            \"exp_var_per\": pca_data.explained_variance_ratio_ * 100,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    x=x_new[:, 0] / x_new[:, 0].max(),\n",
    "    y=x_new[:, 1] / x_new[:, 1].max(),\n",
    "    z=x_new[:, 2] / x_new[:, 2].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLs-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_data_all = PLSRegression(n_components=X.shape[1])\n",
    "x_new = pls_data_all.fit(X, y).transform(X)\n",
    "\n",
    "pls_data_all.score(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=pls_data_all.x_scores_[:, 0] / pls_data_all.x_scores_[:, 0].max(),\n",
    "    y=pls_data_all.x_scores_[:, 1] / pls_data_all.x_scores_[:, 1].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "    ),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    x=pls_data_all.x_scores_[:, 0] / pls_data_all.x_scores_[:, 0].max(),\n",
    "    y=pls_data_all.x_scores_[:, 1] / pls_data_all.x_scores_[:, 1].max(),\n",
    "    z=pls_data_all.x_scores_[:, 2] / pls_data_all.x_scores_[:, 2].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting the scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has not been successful, were going o try switching from a resistance scale to a susceptibility scale, this allows us to keep all dimensions for all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inverted = (\n",
    "    df_merged.assign(\n",
    "        surface_necrosee=lambda x: 10 - x.surface_necrosee,\n",
    "        densite_sporulation=lambda x: 10 - x.densite_sporulation,\n",
    "        taille_necrose=lambda x: 10 - x.taille_necrose,\n",
    "        oiv=lambda x: 10 - x.oiv,\n",
    "    )\n",
    "    .assign(\n",
    "        surface_necrosee=lambda x: x.surface_necrosee.fillna(0),\n",
    "        densite_sporulation=lambda x: x.densite_sporulation.fillna(0),\n",
    "        taille_necrose=lambda x: x.taille_necrose.fillna(0),\n",
    "        sporulation=lambda x: x.sporulation.fillna(0),\n",
    "    )\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\n",
    "        [\n",
    "            \"oiv\",\n",
    "            \"experiment\",\n",
    "            \"sheet\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "df_inverted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a numeric dataframe without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_num = (\n",
    "    df_inverted.drop([\"colonne\"], axis=1)\n",
    "    .select_dtypes(exclude=object)\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_inv_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = make_subplots(rows=1, cols=len(df_inverted.columns))\n",
    "for i, var in enumerate(df_inverted.columns):\n",
    "    fig.add_trace(\n",
    "        go.Violin(y=df_inverted[var], name=var),\n",
    "        row=1,\n",
    "        col=i + 1,\n",
    "    )\n",
    "fig.update_traces(points=\"all\", jitter=0.3).update_layout(\n",
    "    height=1000,\n",
    "    width=1400,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OIV distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    x=df_inv_num.oiv.sort_values().astype(str),\n",
    "    color=df_inv_num.oiv.sort_values().astype(str),\n",
    "    text_auto=True,\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = df_inv_num\n",
    "yi = df_inv_num.oiv\n",
    "Xi = Xi.drop([\"oiv\"], axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xi)\n",
    "Xi = scaler.transform(Xi)\n",
    "\n",
    "Xi.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = PCA()\n",
    "x_new = pca_data.fit_transform(Xi)\n",
    "\n",
    "df_inv_num[\"x_pca\"] = x_new[:, 0]\n",
    "df_inv_num[\"y_pca\"] = x_new[:, 1]\n",
    "df_inv_num[\"z_pca\"] = x_new[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pls-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_data_all_inv = PLSRegression(n_components=Xi.shape[1])\n",
    "x_new = pls_data_all_inv.fit(Xi, yi).transform(Xi)\n",
    "\n",
    "df_inv_num[\"x_pls\"] = pls_data_all_inv.x_scores_[:, 0]\n",
    "df_inv_num[\"y_pls\"] = pls_data_all_inv.x_scores_[:, 1]\n",
    "df_inv_num[\"z_pls\"] = pls_data_all_inv.x_scores_[:, 2]\n",
    "\n",
    "pls_data_all_inv.score(Xi, yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pal = px.colors.qualitative.Plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"PCA\", \"PLS\"])\n",
    "col_pal_iterator = itertools.cycle(col_pal)\n",
    "\n",
    "for i in odd_numbers:\n",
    "    new_colour = next(col_pal_iterator)\n",
    "    df_tmp = df_inv_num[df_inv_num.oiv == i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_tmp[\"x_pca\"],\n",
    "            y=df_tmp[\"y_pca\"],\n",
    "            mode=\"markers\",\n",
    "            # name=f\"OIV {i}\",\n",
    "            text=df_tmp.index,\n",
    "            line=dict(color=new_colour),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    df_tmp = df_inv_num[df_inv_num.oiv == i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_tmp[\"x_pls\"],\n",
    "            y=df_tmp[\"y_pls\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"OIV {i}\",\n",
    "            text=df_tmp.index,\n",
    "            line=dict(color=new_colour),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=800, width=1400, title=\"PCA vs PLS\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"PCA\", \"PLS\"])\n",
    "col_pal_iterator = itertools.cycle(col_pal)\n",
    "\n",
    "for i in odd_numbers:\n",
    "    new_colour = next(col_pal_iterator)\n",
    "    df_tmp = df_inv_num[df_inv_num.oiv == i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=df_tmp[\"x_pca\"],\n",
    "            y=df_tmp[\"y_pca\"],\n",
    "            z=df_tmp[\"z_pca\"],\n",
    "            mode=\"markers\",\n",
    "            # name=f\"OIV {i}\",\n",
    "            text=df_tmp.index,\n",
    "            line=dict(color=new_colour),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    df_tmp = df_inv_num[df_inv_num.oiv == i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=df_tmp[\"x_pls\"],\n",
    "            y=df_tmp[\"y_pls\"],\n",
    "            z=df_tmp[\"z_pls\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"OIV {i}\",\n",
    "            text=df_tmp.index,\n",
    "            line=dict(color=new_colour),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=800, width=1400, title=\"PCA vs PLS\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variance(\n",
    "    df_ev=pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"pc\": [f\"PC{i}\" for i in range(len(pca_data.explained_variance_ratio_))],\n",
    "            \"exp_var_per\": pca_data.explained_variance_ratio_ * 100,\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    x=x_new[:, 0] / x_new[:, 0].max(),\n",
    "    y=x_new[:, 1] / x_new[:, 1].max(),\n",
    "    z=x_new[:, 2] / x_new[:, 2].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pls-da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_data_all = PLSRegression(n_components=X.shape[1])\n",
    "x_new = pls_data_all.fit(X, y).transform(X)\n",
    "\n",
    "pls_data_all.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inv_num[\"x\"] = \n",
    "\n",
    "fig = px.scatter(\n",
    "    x=pls_data_all.x_scores_[:, 0] / pls_data_all.x_scores_[:, 0].max(),\n",
    "    y=pls_data_all.x_scores_[:, 1] / pls_data_all.x_scores_[:, 1].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "    ),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(\n",
    "    x=pls_data_all.x_scores_[:, 0] / pls_data_all.x_scores_[:, 0].max(),\n",
    "    y=pls_data_all.x_scores_[:, 1] / pls_data_all.x_scores_[:, 1].max(),\n",
    "    z=pls_data_all.x_scores_[:, 2] / pls_data_all.x_scores_[:, 2].max(),\n",
    "    color=y.astype(str),\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title=\"PCA 3D\",\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sheet by sheet data Pls-da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_plsda = pd.DataFrame(\n",
    "    columns=[\"experiment\", \"sheet\", \"row_count\", \"score\"]\n",
    ")\n",
    "failures = []\n",
    "\n",
    "for idx, row in df_inverted[[\"experiment\", \"sheet\"]].drop_duplicates().iterrows():\n",
    "    try:\n",
    "        df = (\n",
    "            df_inverted[\n",
    "                (df_inverted.experiment == row[\"experiment\"])\n",
    "                & (df_inverted.sheet == row[\"sheet\"])\n",
    "            ]\n",
    "            .select_dtypes(exclude=object)\n",
    "            .drop([\"colonne\"], axis=1)\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        X = df.drop([\"oiv\"], axis=1)\n",
    "        y = df.oiv\n",
    "        X = StandardScaler().fit(X).transform(X)\n",
    "        cur_pls_da = PLSRegression(n_components=X.shape[1])\n",
    "        cur_pls_da.fit(X, y).transform(X)\n",
    "\n",
    "        df_sheet_plsda = df_sheet_plsda.append(\n",
    "            {\n",
    "                \"experiment\": row[\"experiment\"],\n",
    "                \"sheet\": row[\"sheet\"],\n",
    "                \"row_count\": df.shape[0],\n",
    "                \"score\": cur_pls_da.score(X, df.oiv),\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    except:\n",
    "        failures.append((row[\"experiment\"], row[\"sheet\"]))\n",
    "\n",
    "df_sheet_plsda = df_sheet_plsda.sort_values(\n",
    "    [\n",
    "        \"row_count\",\n",
    "        \"score\",\n",
    "        \"experiment\",\n",
    "        \"sheet\",\n",
    "    ],\n",
    "    ascending=False,\n",
    ").reset_index(drop=True)\n",
    "df_sheet_plsda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    data_frame=df_sheet_plsda[df_sheet_plsda.score > 0],\n",
    "    x=\"row_count\",\n",
    "    y=\"score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_num[(df_inv_num < 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "            df_num[df_num[c] == df_num[c].max]\n",
    "            for c in [\n",
    "                \"necrose\",\n",
    "                \"surface_necrosee\",\n",
    "                \"sporulation\",\n",
    "                \"densite_sporulation\",\n",
    "                \"taille_necrose\",\n",
    "            ]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num[df_num[\"necrose\"] == df_num[\"necrose\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
