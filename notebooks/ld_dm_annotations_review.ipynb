{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import markdown\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "import datapane as dp\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from yellowbrick.cluster import (\n",
    "    KElbowVisualizer,\n",
    "    SilhouetteVisualizer,\n",
    "    InterclusterDistance,\n",
    ")\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import ipywidgets as ipw\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, os.path.join(\"..\", \"scripts\"))\n",
    "\n",
    "import gav_oidium_func as gof\n",
    "import gav_oidium_const as goc\n",
    "import gav_oidium_text as got\n",
    "import gav_oidium_plot_plotly as gop\n",
    "\n",
    "import IPython.display as disp\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image as IpImage\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pd.options.display.float_format = \"{:4,.2f}\".format\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "os.chdir(os.path.abspath(''))\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_mkdw(text):\n",
    "    disp.display(disp.Markdown(text))\n",
    "\n",
    "def disp_img(path):\n",
    "    disp.display(disp.Image(path))\n",
    "\n",
    "def get_yellow_fig(visualizer):\n",
    "    fig = visualizer.fig\n",
    "    ax = visualizer.show()\n",
    "    fig.axes.append(ax)\n",
    "    return visualizer.fig\n",
    "\n",
    "default_plot_height = 600\n",
    "\n",
    "conflict_columns = [\n",
    "    \"sporulation\",\n",
    "    \"necrose\",\n",
    "    \"taille_necrose\",\n",
    "    \"surface_necrosee\",\n",
    "    \"densite_sporulation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_title = f\"{goc.lvl_1_header} Collation and Review of Downy Mildew Annotations\"\n",
    "disp_mkdw(txt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_intro = f\"\"\"\n",
    "{goc.lvl_2_header} Introduction\n",
    "In this document we will collate all available OIV 452 annotation and then review them in the hope of predicting OIV 452 using the new proposed variables. \n",
    "When this fails we will try to understand the issues and propose actions the adress the problems.\n",
    "\n",
    "We will 1) explain what OIV 452 and Downy Mildiou 2) Build a model ready dataframe\n",
    "\"\"\"\n",
    "disp_mkdw(txt_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_def_pm = f\"\"\"\n",
    "{goc.lvl_2_header} Powdery Mildew\n",
    "&ndash; *Powdery Mildew, from [Wikipedia](https://en.wikipedia.org/wiki/Downy_mildew)\n",
    "\n",
    "**Downy mildew** refers to any of several types of oomycete microbes that are obligate parasites of plants. \n",
    "Downy mildews exclusively belong to the Peronosporaceae family. In commercial agriculture, they are a \n",
    "particular problem for growers of crucifers, grapes and vegetables that grow on vines. \n",
    "The prime example is Peronospora farinosa featured in NCBI-Taxonomy and HYP3. \n",
    "This pathogen does not produce survival structures in the northern states of the United States, \n",
    "and overwinters as live mildew colonies in Gulf Coast states. It progresses northward with cucurbit production \n",
    "each spring. Yield loss associated with downy mildew is most likely related to soft rots that occur after plant \n",
    "canopies collapse and sunburn occurs on fruit. Cucurbit downy mildew only affects leaves of cucurbit plants.\n",
    "\n",
    "**Symptoms**: Initial symptoms include large, angular or blocky, yellow areas visible on the upper surface. \n",
    "As lesions mature, they expand rapidly and turn brown. The under surface of infected leaves appears watersoaked. \n",
    "Upon closer inspection, a purple-brown mold (see arrow) becomes apparent. Small spores shaped like footballs can be \n",
    "observed among the mold with a 10x hand lens. In disease-favorable conditions (cool nights with long dew periods), \n",
    "downy mildew will spread rapidly, destroying leaf tissue without affecting stems or petioles.\n",
    "\"\"\"\n",
    "disp_mkdw(txt_def_pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_def_oiv_452_1 = markdown.markdown(\n",
    "    f\"\"\"\n",
    "{goc.lvl_2_header} OIV 452\n",
    "{got.txt_oiv_452_spec}\n",
    "\"\"\"\n",
    ")\n",
    "disp_mkdw(txt_def_oiv_452_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_def_oiv_452_1 = os.path.join(goc.datain_path, \"images\", \"OIV_examples.png\")\n",
    "disp_img(img_def_oiv_452_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_oiv_necrose = f\"\"\"\n",
    "{goc.lvl_3_header} OIV 452-2\n",
    "A new version of the annotation specification added necrosis to the observed traits.\n",
    "\"\"\"\n",
    "disp_mkdw(txt_oiv_necrose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_oiv_necrosis = gop.plot_sample_oiv_images(height=900)\n",
    "plt_oiv_necrosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_new_var_spex = f\"\"\"\n",
    "{goc.lvl_3_header} New variables\n",
    "{got.txt_what_we_want}\n",
    "\"\"\"\n",
    "\n",
    "img_new_var_spex = os.path.join(goc.datain_path, \"images\", \"oiv_452-1_desc.png\")\n",
    "\n",
    "disp_mkdw(txt_new_var_spex)\n",
    "disp_img(img_new_var_spex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_intro = f\"\"\"{goc.lvl_2_header} Build dataframe\n",
    "\n",
    "Experiment data is divided by year and ewperiment, in order to proceed to model building we need to first collate all darta\n",
    "\n",
    "{goc.lvl_3_header} Locating the files\n",
    "Files containing experiment's phenotyping data are stored by year and experiment, the data files are \n",
    "Excel classifiers which contain the word \"saisie\", \n",
    "we're going to parse all the folders year by year and retrieve the files.\n",
    "\n",
    "- Files containing DM for downy mildew, ie mildiou, are selected for OIV analysis\n",
    "- Files containing PM for powdery mildew, ie o√Ødium, are discarded\n",
    "\n",
    "{goc.lvl_3_header} Extracting data from the sheets\n",
    "Each Excel file contains one or more sheets. We parse all available sheets and discard them if:\n",
    "- A valid header is not found\n",
    "- The dataframe within the sheet is not valid\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_bdf_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gof.get_distant_excels()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gof.copy_excel_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = gof.filter_csvs()\n",
    "\n",
    "lcl_csv_files = [\n",
    "    os.path.join(goc.oidium_extracted_csvs_path, filename)\n",
    "    for filename in df_result.csv_file_name.dropna().to_list()\n",
    "]\n",
    "\n",
    "txt_bdf_sample_sheet_header = \"Sample of the first sheet\"\n",
    "\n",
    "df_bdf_sample_sheet_df = pd.read_csv(lcl_csv_files[0])\n",
    "\n",
    "\n",
    "disp_mkdw(txt_bdf_sample_sheet_header)\n",
    "df_bdf_sample_sheet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_filtering_review_title = f\"{goc.lvl_3_header} Filtereing review\"\n",
    "\n",
    "txt_bdf_filtering_review = f\"\"\"\n",
    "After checking all the available sheets we end up with {df_result[df_result.comment == 'success'].shape[0]} valid sheets.\n",
    "\n",
    "{got.txt_rejected_csvs}\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_bdf_filtering_review_title)\n",
    "disp_mkdw(txt_bdf_filtering_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dfb_filtering_result = gop.plot_rejected_hist(df_result)\n",
    "plot_dfb_filtering_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bdf_filtering_outcome = gof.sheet_filtering_out_df(df_result)\n",
    "df_bdf_filtering_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_why_rejected = got.txt_rejected_csvs\n",
    "disp_mkdw(txt_bdf_why_rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_merge_sheets_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Merging all sheets into a single dataframe\n",
    "We are going to merge all the sheets into a single dataframe. Since all the phenotyping was written manually we're to check cata consistency with the following rules \n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_merge_sheets_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_consistency_check = f\"\"\"\n",
    "{goc.lvl_4_header} Consistency rules\n",
    "{got.txt_oiv_452_spec_req}\n",
    "The data histogram shows that there are numerous inconsistencies in the data:\n",
    "- Variables are not always limited to their set values\n",
    "- Variables are inconsistent within themselves, ie. sporulation may be set to 1 with an OIV 9 which is impossible since an OIV 9 means no sporulation at all\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_consistency_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_steps = {}\n",
    "df_raw_merged = gof.build_raw_merged(lcl_csv_files)\n",
    "clean_steps[\"raw_merge\"] = (df_raw_merged.shape[0], 0)\n",
    "\n",
    "plot_bdf_inconsistency_raw = gop.plot_inconsistencies(\n",
    "    df_raw_merged,\n",
    "    sort_values=False,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "plot_bdf_inconsistency_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_consistency_error = f\"\"\"\n",
    "{goc.lvl_4_header} Consistency errors overview\n",
    "First we're going to check what are the inconsistencies and how often do they appear\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_consistency_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_inconsistency_legend = f\"\"\"\n",
    "{goc.lvl_5_header} Sheets with inconsistent data\n",
    "- **oob**: Out of bounds, value outside of permitted values\n",
    "- **n_inc**: Linked values inconsistent\n",
    "\n",
    "\"\"\"\n",
    "df_bdf_inconsistent = gof.build_inconsistencies_dataframe(df_raw_merged)\n",
    "\n",
    "disp_mkdw(txt_bdf_inconsistency_legend)\n",
    "df_bdf_inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"sporulation_oob\",\n",
    "    \"sporulation_ds_inc\",\n",
    "    \"densite_sporulation_oob\",\n",
    "    \"necrose_oob\",\n",
    "    \"necrose_sn_inc\",\n",
    "    \"necrose_tn_inc\",\n",
    "    \"taille_necrose_oob\",\n",
    "    \"surface_necrosee_oob\",\n",
    "    \"oiv_oob\",\n",
    "    \"oiv_s_inc\",\n",
    "    \"ligne_oob\",\n",
    "]\n",
    "\n",
    "df_bdf_inconsistency_count = pd.DataFrame(\n",
    "    data={\"Inconsistency type count\": [df_bdf_inconsistent[col].sum() for col in cols]},\n",
    "    index=cols,\n",
    ")\n",
    "\n",
    "df_bdf_inconsistency_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_consistent_dataframe = f\"\"\"\n",
    "{goc.lvl_5_header} Consistent dataframe\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_consistent_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = gof.clean_merged_dataframe(df_raw_merged)\n",
    "clean_steps[\"clean_raw_merge\"] = (\n",
    "    df_merged.shape[0],\n",
    "    df_raw_merged.shape[0] - df_merged.shape[0],\n",
    ")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "txt_bdf_count_after_clean = f\"After removing inconsistent lines we went from {df_raw_merged.shape[0]} to {df_merged.shape[0]} consistent rows\"\n",
    "disp_mkdw(txt_bdf_count_after_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_inconsistency_clean = gop.plot_inconsistencies(\n",
    "    df_merged,\n",
    "    height=900,\n",
    "    title=\"No visible errors remain in the data\",\n",
    ")\n",
    "plot_bdf_inconsistency_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_data_overview_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Data overview\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_data_overview_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_data_overview_set_balance = f\"{goc.lvl_4_header} Set balance\"\n",
    "disp_mkdw(txt_bdf_data_overview_set_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_data_overview_set_balance = gop.plot_balance_histogram(\n",
    "    labels=df_merged.oiv.sort_values().astype(str),\n",
    "    color=df_merged.oiv.sort_values().astype(str),\n",
    "    is_text=True,\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_bdf_data_overview_set_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_data_overview_nan_values = f\"\"\"\n",
    "{goc.lvl_4_header} NaN values\n",
    "NaN values happen when:\n",
    "- If \"necrosis\" is 0, \"surface_necrosee\" and \"taille_necrose\" are NaN\n",
    "- If \"sporulation\" is 0, \"densite_sporulation\" is NaN\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_data_overview_nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bdf_data_overview_nan_values = pd.DataFrame(\n",
    "    data={\"NaN count\": [df_merged[c].isna().sum() for c in df_merged.columns]},\n",
    "    index=df_merged.columns,\n",
    ").sort_values(by=[\"NaN count\"], ascending=False)\n",
    "\n",
    "df_bdf_data_overview_nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_numeric_dataframe_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Numeric dataframe\n",
    "We remove all columns that are not usefull for a classification model, such as column, line, etc... and drop all rows with **NaN** values \n",
    "as they **will not be accepted by the models**.\n",
    "After removing this columns there will be duplicates that are removed.\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_numeric_dataframe_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = (\n",
    "    df_merged.drop([\"colonne\"], axis=1)\n",
    "    .dropna()\n",
    "    .select_dtypes(exclude=object)\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_num_cols = df_num.columns\n",
    "df_num_cols = [\n",
    "    df_num_cols[3],\n",
    "    df_num_cols[0],\n",
    "    df_num_cols[2],\n",
    "    df_num_cols[4],\n",
    "    df_num_cols[1],\n",
    "    df_num_cols[5],\n",
    "]\n",
    "df_num = df_num[df_num_cols].sort_values([\"oiv\", \"sporulation\", \"necrose\"])\n",
    "\n",
    "clean_steps[\"numeric_dataframe\"] = (\n",
    "    df_num.shape[0],\n",
    "    df_merged.shape[0] - df_num.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_numeric_dataframe_new_set_balance = f\"{goc.lvl_4_header} New set balance\"\n",
    "plot_bdf_numeric_dataframe_new_set_balance = gop.plot_balance_histogram(\n",
    "    labels=df_num.oiv.sort_values().astype(str),\n",
    "    color=df_num.oiv.sort_values().astype(str),\n",
    "    is_text=True,\n",
    "    height=default_plot_height,\n",
    ")\n",
    "disp_mkdw(txt_bdf_numeric_dataframe_new_set_balance)\n",
    "plot_bdf_numeric_dataframe_new_set_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_numeric_dataframe_no_9 = f\"\"\"\n",
    "Since OIV 9 implies no sporulation, there are no longer rows with OIV value 9\n",
    "There are only **{df_num.shape[0]}** observations left, two hypothesis:\n",
    "- There are only this amount of phenotypes possible\n",
    "- The human eye can only discriminate this many\n",
    "\n",
    "**There are no rows with OIV 9, this makes building a model pointless, we'll find another way**\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_numeric_dataframe_no_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_invert_axis = f\"\"\"\n",
    "{goc.lvl_3_header} {got.txt_lvl2_header_invert_axes}\n",
    "{got.txt_fail}\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_invert_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inverted = gof.invert_axis(df_merged)\n",
    "\n",
    "df_inverted = df_inverted[\n",
    "    [df_inverted.columns[i] for i in [9, 8, 4, 6, 0, 3, 2, 10, 5, 7, 1]]\n",
    "].sort_values([\"oiv\", \"sporulation\", \"necrose\"])\n",
    "\n",
    "txt_bdf_invert_axis_df_head = f\"{goc.lvl_4_header} The head of new dataframe\"\n",
    "txt_bdf_invert_axis_df_shape = str(df_inverted.shape)\n",
    "\n",
    "disp_mkdw(txt_bdf_invert_axis_df_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inverted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disp_mkdw(txt_bdf_invert_axis_df_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_num = (\n",
    "    df_inverted.drop([\"colonne\"], axis=1)\n",
    "    .select_dtypes(exclude=object)\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\n",
    "        [\n",
    "            \"oiv\",\n",
    "            \"necrose\",\n",
    "            \"taille_necrose\",\n",
    "            \"surface_necrosee\",\n",
    "            \"sporulation\",\n",
    "            \"densite_sporulation\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_steps[\"inverted_numeric_dataframe\"] = (\n",
    "    df_inv_num.shape[0],\n",
    "    df_merged.shape[0] - df_inv_num.shape[0],\n",
    ")\n",
    "\n",
    "txt_bdf_invert_axis_df_inv_shape = str(df_inv_num.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_invert_axis_df_inv_head = f\"\"\"\n",
    "{goc.lvl_4_header} The numeric dataframe\n",
    "We remove some columns from dataframe as they contain metadata that does not contribute to the OIV classification. After this operation we obtain the final dataframe used to build the models.\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_bdf_invert_axis_df_inv_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(txt_bdf_invert_axis_df_inv_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_overview = f\"{goc.lvl_4_header} Final dataset overview\"\n",
    "txt_bdf_fdf_overview_sankey = f\"{goc.lvl_5_header} Evolution of available rows\"\n",
    "plot_bdf_fdf_overview_sankey = gop.observations_sankey(\n",
    "    clean_steps=clean_steps,\n",
    "    width=None,\n",
    "    height=default_plot_height,\n",
    ")\n",
    "txt_bdf_fdf_overview_sankey_explain = f\"\"\"\n",
    "We started with {df_raw_merged.shape[0]} annotations and after removing inconsistent data, \n",
    "columns that are not needed and duplicates we end up with {df_inv_num.shape[0]} observations\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_bdf_fdf_overview)\n",
    "disp_mkdw(txt_bdf_fdf_overview_sankey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_fdf_overview_sankey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(txt_bdf_fdf_overview_sankey_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_overview_balance = f\"{goc.lvl_5_header} New set balance\"\n",
    "disp_mkdw(txt_bdf_fdf_overview_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_fdf_overview_balance = px.histogram(\n",
    "    x=df_inv_num.oiv.sort_values().astype(str),\n",
    "    color=df_inv_num.oiv.sort_values().astype(str),\n",
    "    text_auto=True,\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_bdf_fdf_overview_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_overview_outcome = f\"\"\"\n",
    "{goc.lvl_4_header} Result\n",
    "There are {df_inv_num.shape[0]} observations left instead of the previous {df_num.shape[0]}\n",
    "Two hypothesis:\n",
    "- There are only this amount of phenotypes possible\n",
    "- The human eye can only discriminate this many\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_fdf_overview_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_overview_cm = f\"{goc.lvl_5_header} Correlation matrix\"\n",
    "disp_mkdw(txt_bdf_fdf_overview_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_fdf_overview_cm = px.imshow(\n",
    "    df_inv_num.drop_duplicates().corr(),\n",
    "    text_auto=True,\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_bdf_fdf_overview_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_homogeinity_intro = f\"\"\"\n",
    "{goc.lvl_5_header} Data boxplot/heatmap per OIV score and average\n",
    "Plotting a heat of all the variables against each OIV to see if we can detect clusters visualy.\n",
    "\"\"\"\n",
    "\n",
    "plot_bdf_fdf_vclusters_hm = [\n",
    "    gop.plot_oiv_homogeneity(\n",
    "        df_src=df_inv_num,\n",
    "        oiv=i,\n",
    "        height=400,\n",
    "    )\n",
    "    for i in [1, 3, 5, 7, 9]\n",
    "]\n",
    "\n",
    "plot_bdf_fdf_vclusters_hm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_fdf_vclusters_bp = [\n",
    "    df_inv_num[df_inv_num.oiv == i]\n",
    "    .drop(\"oiv\", axis=1)\n",
    "    .boxplot()\n",
    "    .update_layout(title=f\"OIV {i}\", height=400)\n",
    "    for i in [1, 3, 5, 7, 9]\n",
    "]\n",
    "plot_bdf_fdf_vclusters_bp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bdf_fdf_vclusters_violin = [\n",
    "    px.violin(\n",
    "        df_inv_num,\n",
    "        color=\"oiv\",\n",
    "        y=col,\n",
    "        box=True,\n",
    "    )\n",
    "    for col in conflict_columns\n",
    "]\n",
    "\n",
    "plot_bdf_fdf_vclusters_violin[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_averages_intro = f\"\"\"\n",
    "Variables averages per OIV value\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_bdf_fdf_averages_intro)\n",
    "plot_bdf_fdf_vclusters_avg = gop.plot_avg_by_oiv(df_inv_num, height=400)\n",
    "plot_bdf_fdf_vclusters_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_averages_out = f\"\"\"\n",
    "Only \"densite_sporulation\" loks corelated to OIV but only at -O.7\n",
    "\"\"\"\n",
    "disp_mkdw(txt_bdf_fdf_averages_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_homogeinity = got.txt_homogenity_txt\n",
    "disp_mkdw(txt_bdf_fdf_homogeinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_bdf_fdf_homogeinity_means = got.txt_homogenity_avg_txt\n",
    "disp_mkdw(txt_bdf_fdf_homogeinity_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_models_intro = f\"{goc.lvl_2_header} Predicting OIV with the other variables\"\n",
    "\n",
    "Xi = df_inv_num\n",
    "yi = df_inv_num.oiv.astype(int)\n",
    "Xi = Xi.drop([\"oiv\"], axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xi)\n",
    "Xi = scaler.transform(Xi)\n",
    "\n",
    "disp_mkdw(txt_models_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_model_def_pca)\n",
    "plot_model_pca = gop.plot_model(\n",
    "    X=PCA().fit_transform(Xi),\n",
    "    color=yi.astype(str),\n",
    "    title=\"Inverted PCA 2D\",\n",
    "    # height=default_plot_height,\n",
    ")\n",
    "plot_model_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_model_def_plsda)\n",
    "\n",
    "pls_data_all_inv = PLSRegression(n_components=Xi.shape[1])\n",
    "x_new = pls_data_all_inv.fit(Xi, yi).transform(Xi)\n",
    "\n",
    "plot_model_def_plsda = gop.plot_model(\n",
    "    X=x_new,\n",
    "    color=yi.astype(str),\n",
    "    title=f\"Inverted PLS-DA, score: {pls_data_all_inv.score(Xi, yi)}\",\n",
    "    axis_title_root=\"X-variate \",\n",
    "    # height=default_plot_height,\n",
    ")\n",
    "plot_model_def_plsda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_model_def_lda)\n",
    "lda_data_all_inv = LinearDiscriminantAnalysis()\n",
    "x_new = lda_data_all_inv.fit(Xi, yi).transform(Xi)\n",
    "plot_model_def_lda = gop.plot_model(\n",
    "    X=x_new,\n",
    "    color=yi.astype(str),\n",
    "    title=f\"Inverted LDA score: {lda_data_all_inv.score(Xi, yi)}\",\n",
    "    axis_title_root=\"X-variate \",\n",
    "    # height=default_plot_height,\n",
    ")\n",
    "plot_model_def_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model_check_overlap_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Check overlapping\n",
    "Some observations seem to overlap, we're going to check that one point in the vectorial space codes only one OIV\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_lines = (\n",
    "    df_inv_num.drop([\"oiv\"], axis=1)\n",
    "    .drop_duplicates(conflict_columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_dup = pd.DataFrame(columns=conflict_columns + [f\"oiv {oiv}\" for oiv in [1,3,5,7,9]] + [\"OIV count\"])\n",
    "for i in range(df_unique_lines.shape[0]):\n",
    "    tmp_df = df_unique_lines.iloc[[i]]\n",
    "    oivs = pd.merge(left=tmp_df, right=df_inv_num).oiv.to_list()\n",
    "    for oiv in oivs:\n",
    "        tmp_df[f\"oiv {oiv}\"] = True\n",
    "    tmp_df[\"OIV count\"] = len(oivs)\n",
    "    df_dup = df_dup.reset_index(drop=True).append(tmp_df, ignore_index=True)\n",
    "\n",
    "txt_model_conflict_row = f\"{goc.lvl_4_header} Conflicted rows for OIV coding\"\n",
    "\n",
    "disp_mkdw(txt_model_conflict_row)\n",
    "df_dup[df_dup[\"OIV count\"] > 1].sort_values([\"OIV count\"], ascending=False).replace(np.NaN, \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model_conflict_loc = f\"\"\"\n",
    "{goc.lvl_4_header} Where are the conflicts\n",
    "We're vgoing to plot the observation in the latent space of a PCA to visualize where they are located.\n",
    "\"\"\"\n",
    "\n",
    "df = df_dup.sort_values([\"OIV count\"])\n",
    "color = df[\"OIV count\"]\n",
    "df = df[conflict_columns]\n",
    "\n",
    "plot_model_conflict_locs = [\n",
    "    gop.plot_pca(\n",
    "        pca=PCA(),\n",
    "        df=df,\n",
    "        pcx=pcx,\n",
    "        pcy=pcy,\n",
    "        title=f\"PCA for PC{pcx + 1}, PC{pcy + 1} with color by coding count\",\n",
    "        pca_columns=conflict_columns,\n",
    "        color=color,\n",
    "    )\n",
    "    for pcx, pcy in [(0, 1), (0, 2), (1, 2)]\n",
    "]\n",
    "\n",
    "disp_mkdw(txt_model_conflict_loc)\n",
    "plot_model_conflict_locs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model_sbs_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Sheet by sheet prediction\n",
    "The prediction is bad at {pls_data_all_inv.score(Xi, yi)}, we try next to predict sheet by sheet to see the results\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_model_sbs_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbs_plsda = (\n",
    "    gof.build_sbs_plsda(df_inverted, gof.build_dup_df(df_inv_num)[\"df_dup\"])\n",
    "    .sort_values(\n",
    "        [\n",
    "            \"row_count\",\n",
    "            \"score\",\n",
    "            \"experiment\",\n",
    "            \"sheet\",\n",
    "        ],\n",
    "        ascending=False,\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_sbs_plsda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sbs_plsda = px.scatter(\n",
    "    data_frame=df_sbs_plsda[\n",
    "        ((df_sbs_plsda.score > -1) & (df_sbs_plsda.score <= 1))\n",
    "    ].assign(row_count=lambda x: x.row_count.astype(float)),\n",
    "    y=\"score\",\n",
    "    x=\"dup_rate\",\n",
    "    color=\"row_count\",\n",
    "    color_continuous_scale=px.colors.sequential.OrRd,\n",
    "    trendline=\"ols\",\n",
    "    trendline_color_override=\"blue\",\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_sbs_plsda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_duprate_vs_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model_rem_var_intro = f\"\"\"\n",
    "{goc.lvl_3_header} Removing some variables from the dataset\n",
    "Necrosis and sporulation are heavily linked to the other variables, we will test models build without them\n",
    "\"\"\"\n",
    "\n",
    "df_inv_num_wosn = (\n",
    "    df_inv_num[[\"taille_necrose\", \"surface_necrosee\", \"densite_sporulation\", \"oiv\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "yi_wond = df_inv_num_wosn.oiv\n",
    "X_wond = df_inv_num_wosn.drop([\"oiv\"], axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_wond)\n",
    "X_wond = scaler.transform(X_wond)\n",
    "\n",
    "disp_mkdw(txt_model_rem_var_intro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rem_var_pca =gop.plot_model(\n",
    "    X=PCA().fit_transform(X_wond),\n",
    "    color=yi_wond.astype(str),\n",
    "    title=\"Inverted PCA 2D without sporulation nor necrosis\",\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_model_rem_var_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_data_all_inv = PLSRegression(n_components=X_wond.shape[1])\n",
    "x_new = pls_data_all_inv.fit(X_wond, yi_wond).transform(X_wond)\n",
    "plot_model_rem_var_plsda = gop.plot_model(\n",
    "    X=pls_data_all_inv.x_scores_,\n",
    "    color=yi_wond.astype(str),\n",
    "    title=f\"Inverted PLS-DA without sporulation nor necrosis, score: {pls_data_all_inv.score(X_wond, yi_wond)}\",\n",
    "    axis_title_root=\"X-variate \",\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_model_rem_var_plsda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_model_rem_var_outro = \"No visible change\"\n",
    "disp_mkdw(txt_model_rem_var_outro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_noiv_header = f\"\"\"\n",
    "{goc.lvl_2_header} {got.txt_lvl2_header_kmeans}\n",
    "{got.txt_kmeans}\n",
    "{got.txt_noiv_sel_cut}\n",
    "{got.txt_noiv_sel_cut_outcome}\n",
    "\"\"\"\n",
    "disp_mkdw(txt_noiv_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_km = df_inv_num.drop([\"oiv\"], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "plot_noiv_cut = make_subplots(rows=2, cols=3)\n",
    "\n",
    "for (r, c), sort_order in zip(\n",
    "    itertools.product([1, 2, 3], [1, 2, 3]),\n",
    "    itertools.permutations(\n",
    "        [\"taille_necrose\", \"surface_necrosee\", \"densite_sporulation\"]\n",
    "    ),\n",
    "):\n",
    "    plot_noiv_cut.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=X_km.drop([\"sporulation\", \"necrose\"], axis=1)\n",
    "            .sort_values(list(sort_order))\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True),\n",
    "            x=sort_order,\n",
    "        ),\n",
    "        row=r,\n",
    "        col=c,\n",
    "    )\n",
    "\n",
    "plot_noiv_cut.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=[0, 1, 2],\n",
    "        ticktext=sort_order,\n",
    "    ),\n",
    "    height=800,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    ")\n",
    "plot_noiv_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_kmeans_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xkm_pca = PCA()\n",
    "x_pca = xkm_pca.fit_transform(X_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_pca = fig = px.scatter_3d(\n",
    "    x=x_pca[:, 0],\n",
    "    y=x_pca[:, 1],\n",
    "    z=x_pca[:, 2],\n",
    "    title=\"PCA\",\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_noiv_kmeans_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_pca_variance = gop.plot_variance(\n",
    "    df_ev=pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"pc\": [\n",
    "                f\"PC{i}\"\n",
    "                for i in range(len(xkm_pca.explained_variance_ratio_))\n",
    "            ],\n",
    "            \"exp_var_per\": xkm_pca.explained_variance_ratio_ * 100,\n",
    "        }\n",
    "    ),\n",
    "    height=default_plot_height,\n",
    ")\n",
    "plot_noiv_kmeans_pca_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loadings: pd.DataFrame = pd.DataFrame(\n",
    "    xkm_pca.components_.T * xkm_pca.explained_variance_ratio_,\n",
    "    columns=[f\"PC{i+1}\" for i in range(len(xkm_pca.components_))],\n",
    "    index=X_km.columns,\n",
    ")\n",
    "plot_noiv_kmeans_pca_loadings = df_loadings.T.plot.bar()\n",
    "plot_noiv_kmeans_pca_loadings.update_layout(\n",
    "    height=default_plot_height,\n",
    "    title=\"Loadings\",\n",
    ")\n",
    "plot_noiv_kmeans_pca_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_noiv_pca_outcome = \"It appears that **3** components are enough\"\n",
    "disp_mkdw(txt_noiv_pca_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_list = [\n",
    "    px.scatter_3d(\n",
    "        data_frame=pd.DataFrame(\n",
    "            {\n",
    "                \"x\": x_pca[:, 0],\n",
    "                \"y\": x_pca[:, 1],\n",
    "                \"z\": x_pca[:, 2],\n",
    "                \"color\": KMeans(n_clusters=nc, init=\"k-means++\", random_state=42)\n",
    "                .fit_predict(x_pca)\n",
    "                .astype(int)\n",
    "                .astype(str),\n",
    "            }\n",
    "        ).sort_values([\"color\"]),\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        z=\"z\",\n",
    "        height=400,\n",
    "        color=\"color\",\n",
    "        title=f\"{nc} classes\"\n",
    "    )\n",
    "    for nc in range(2, 11)\n",
    "]\n",
    "\n",
    "disp_mkdw(got.txt_kmeans_explore_cluster_count)\n",
    "plot_noiv_kmeans_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_kmeans_elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_model = KMeans(init=\"k-means++\", random_state=42)\n",
    "elb_visualizer = KElbowVisualizer(elbow_model, k=(2, 13))\n",
    "elb_visualizer.fit(x_pca)\n",
    "\n",
    "get_yellow_fig(elb_visualizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_kmeans_silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_silhouette_list = [\n",
    "    get_yellow_fig(\n",
    "        SilhouetteVisualizer(\n",
    "            KMeans(init=\"k-means++\", n_clusters=nc, random_state=42)\n",
    "        ).fit(x_pca)\n",
    "    )\n",
    "    for nc in range(2, 11)\n",
    "]\n",
    "plot_noiv_kmeans_silhouette_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_kmeans_outcome)\n",
    "disp_mkdw(got.txt_icdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_icdm = [\n",
    "    get_yellow_fig(\n",
    "        InterclusterDistance(\n",
    "            KMeans(init=\"k-means++\", n_clusters=nc, random_state=42)\n",
    "        ).fit(x_pca)\n",
    "    )\n",
    "    for nc in [3, 6, 8]\n",
    "]\n",
    "\n",
    "plot_noiv_kmeans_icdm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_noiv_select_oiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noiv_kmeans_heat_map = [\n",
    "    px.imshow(\n",
    "        (\n",
    "            X_km.assign(\n",
    "                noiv=KMeans(n_clusters=3, init=\"k-means++\", random_state=42)\n",
    "                .fit_predict(x_pca)\n",
    "                .astype(int)\n",
    "            )\n",
    "            .drop([\"sporulation\", \"necrose\"], axis=1)\n",
    "            .drop_duplicates()\n",
    "            .sort_values(\n",
    "                [\n",
    "                    \"noiv\",\n",
    "                    \"taille_necrose\",\n",
    "                    \"surface_necrosee\",\n",
    "                    \"densite_sporulation\",\n",
    "                ]\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "        )[\n",
    "            [\n",
    "                \"noiv\",\n",
    "                \"taille_necrose\",\n",
    "                \"surface_necrosee\",\n",
    "                \"densite_sporulation\",\n",
    "            ]\n",
    "        ],\n",
    "        height=400,\n",
    "    )\n",
    "    for i in range(2, 11)\n",
    "]\n",
    "plot_noiv_kmeans_heat_map[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_mkdw(got.txt_km_hm_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_noiv_conclusion = f\"\"\"\n",
    "{goc.lvl_2_header} {got.txt_lvl2_header_conclusion}\n",
    "{got.txt_conclusion}\n",
    "\"\"\"\n",
    "\n",
    "disp_mkdw(txt_noiv_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.Report(\n",
    "    dp.Page(\n",
    "        title=txt_title.replace(\"#\", \"\"),\n",
    "        blocks=[\n",
    "            dp.Text(txt_title),\n",
    "            dp.Group(\n",
    "                dp.Media(\n",
    "                    file=os.path.join(\n",
    "                        goc.datain_path,\n",
    "                        \"images\",\n",
    "                        \"oiv_samples\",\n",
    "                        \"smp_oiv_5.png\",\n",
    "                    )\n",
    "                ),\n",
    "                # dp.Plot(plot_bdf_fdf_overview_sankey),\n",
    "                dp.Plot(elb_visualizer.fig),\n",
    "                columns=2,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    dp.Page(title=\"Introduction\", blocks=[dp.Text(txt_intro)]),\n",
    "    dp.Page(\n",
    "        title=\"Definitions\",\n",
    "        blocks=[\n",
    "            dp.Text(txt_def_pm),\n",
    "            dp.Group(\n",
    "                dp.Text(txt_def_oiv_452_1),\n",
    "                dp.Media(file=img_def_oiv_452_1),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_oiv_necrose),\n",
    "            dp.Plot(plt_oiv_necrosis),\n",
    "            dp.Group(\n",
    "                dp.Text(txt_new_var_spex),\n",
    "                dp.Media(file=img_new_var_spex),\n",
    "                columns=2,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    dp.Page(\n",
    "        title=\"Build Dataframe\",\n",
    "        blocks=[\n",
    "            dp.Text(txt_bdf_intro),\n",
    "            # dp.Text(txt_bdf_sample_sheet_header),\n",
    "            # dp.DataTable(df_bdf_sample_sheet_df.head(20)),\n",
    "            dp.Text(txt_bdf_filtering_review_title),\n",
    "            dp.Group(\n",
    "                dp.Text(txt_bdf_filtering_review),\n",
    "                dp.Select(\n",
    "                    blocks=[\n",
    "                        dp.Plot(\n",
    "                            plot_dfb_filtering_result,\n",
    "                            label=\"Filtering result plot\",\n",
    "                        ),\n",
    "                        dp.DataTable(\n",
    "                            df_bdf_filtering_outcome,\n",
    "                            label=\"Filtering result errors\",\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_bdf_merge_sheets_intro),\n",
    "            dp.Group(\n",
    "                dp.Plot(plot_bdf_inconsistency_raw),\n",
    "                dp.Text(txt_bdf_consistency_check),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Select(\n",
    "                dp.Text(\"_\", label=\"No info\"),\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_consistency_error),\n",
    "                    dp.Text(txt_bdf_inconsistency_legend),\n",
    "                    dp.Select(\n",
    "                        dp.DataTable(df_bdf_inconsistent, label=\"Inconsistencies per sheet\"),\n",
    "                        dp.DataTable(df_bdf_inconsistency_count, label=\"Inconsistencies count\"),\n",
    "                    ),\n",
    "                    label=\"Consistency errors overview\"\n",
    "                )\n",
    "            ),            \n",
    "            dp.Text(txt_bdf_consistent_dataframe),\n",
    "            dp.Select(\n",
    "                dp.Plot(\n",
    "                    plot_bdf_inconsistency_clean,\n",
    "                    label=\"Inconsistency check after cleanup\",\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    dp.DataTable(df_merged),\n",
    "                    label=\"New dataframe\",\n",
    "                ),\n",
    "            ),\n",
    "            dp.Text(txt_bdf_count_after_clean),\n",
    "            dp.Text(txt_bdf_data_overview_intro),\n",
    "            dp.Group(\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_data_overview_set_balance),\n",
    "                    dp.Plot(plot_bdf_data_overview_set_balance),\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_data_overview_nan_values),\n",
    "                    dp.DataTable(df_bdf_data_overview_nan_values),\n",
    "                ),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_bdf_numeric_dataframe_intro),\n",
    "            dp.Group(\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_numeric_dataframe_new_set_balance),\n",
    "                    dp.Plot(plot_bdf_numeric_dataframe_new_set_balance),\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_numeric_dataframe_no_9),\n",
    "                ),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_bdf_invert_axis),\n",
    "            dp.Group(\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_invert_axis_df_head),\n",
    "                    dp.DataTable(df_inverted),\n",
    "                    dp.Text(txt_bdf_invert_axis_df_shape),\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    dp.Text(txt_bdf_invert_axis_df_inv_head),\n",
    "                    dp.DataTable(df_inv_num),\n",
    "                    dp.Text(txt_bdf_invert_axis_df_inv_shape),\n",
    "                ),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_bdf_fdf_overview),\n",
    "            # dp.Text(txt_bdf_fdf_overview_sankey),\n",
    "            # dp.Plot(plot_bdf_fdf_overview_sankey),\n",
    "            # dp.Text(txt_bdf_fdf_overview_sankey_explain),\n",
    "            dp.Group(\n",
    "                dp.Text(txt_bdf_fdf_overview_outcome),\n",
    "                dp.Select(\n",
    "                    dp.Plot(plot_bdf_fdf_overview_balance, label=\"New set balance\"),\n",
    "                    dp.Plot(plot_bdf_fdf_overview_cm, label=\"Correlation matrix\"),\n",
    "                ),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_bdf_fdf_homogeinity_intro),\n",
    "            dp.Select(\n",
    "                dp.Group(\n",
    "                    *plot_bdf_fdf_vclusters_bp,\n",
    "                    plot_bdf_fdf_vclusters_avg,\n",
    "                    label=\"Box plot\",\n",
    "                    columns=3\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    *plot_bdf_fdf_vclusters_hm,\n",
    "                    plot_bdf_fdf_vclusters_avg,\n",
    "                    label=\"Heat maps\",\n",
    "                    columns=3\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    *plot_bdf_fdf_vclusters_violin,\n",
    "                    plot_bdf_fdf_vclusters_avg,\n",
    "                    label=\"Violin plots\",\n",
    "                    columns=3\n",
    "                ),\n",
    "            ),\n",
    "            # dp.Text(txt_bdf_fdf_averages_out),\n",
    "        ],\n",
    "    ),\n",
    "    dp.Page(\n",
    "        title=\"Models\",\n",
    "        blocks=[\n",
    "            dp.Text(f\"\"\"\n",
    "                {goc.lvl_2_header} Models\n",
    "                In this section we're going to try to find a relation between the newly added variables and the OIV value.\n",
    "            \"\"\"),\n",
    "            dp.Select(\n",
    "                dp.Group(\n",
    "                    dp.Plot(plot_model_pca),\n",
    "                    dp.Plot(plot_model_def_plsda),\n",
    "                    dp.Plot(plot_model_def_lda),\n",
    "                    columns=3,\n",
    "                    label=\"Model plots\",\n",
    "                ),\n",
    "                dp.Group(\n",
    "                    dp.Text(got.txt_model_def_pca),\n",
    "                    dp.Text(got.txt_model_def_plsda),\n",
    "                    dp.Text(got.txt_model_def_lda),\n",
    "                    columns=3,\n",
    "                    label=\"Model definitions\",\n",
    "                ),\n",
    "            ),\n",
    "            dp.Text(txt_model_check_overlap_intro),\n",
    "            dp.Text(txt_model_conflict_row),\n",
    "            dp.DataTable(df_dup),\n",
    "            dp.Text(txt_model_conflict_loc),\n",
    "            dp.Group(*plot_model_conflict_locs, columns=3),\n",
    "            dp.Text(txt_model_sbs_intro),\n",
    "            dp.Select(\n",
    "                dp.Plot(plot_sbs_plsda, label=\"Sheet by sheet prediction\"),\n",
    "                dp.DataTable(df_sbs_plsda, label=\"Duplicate rate over prediction score\"),\n",
    "            ),\n",
    "            dp.Text(got.txt_duprate_vs_prediction),\n",
    "            dp.Text(txt_model_rem_var_intro),\n",
    "            dp.Group(\n",
    "                dp.Plot(plot_model_rem_var_plsda),\n",
    "                dp.Plot(plot_model_rem_var_pca),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(txt_model_rem_var_outro),\n",
    "        ],\n",
    "    ),\n",
    "    dp.Page(\n",
    "        title=\"OIV alternative\",\n",
    "        blocks=[\n",
    "            dp.Text(txt_noiv_header),\n",
    "            dp.Plot(plot_noiv_cut),\n",
    "            dp.Text(got.txt_kmeans_pca),\n",
    "            dp.Group(\n",
    "                dp.Plot(plot_noiv_kmeans_pca),\n",
    "                dp.Plot(plot_noiv_kmeans_pca_variance),\n",
    "                dp.Plot(plot_noiv_kmeans_pca_loadings),\n",
    "                columns=3,\n",
    "            ),\n",
    "            dp.Text(txt_noiv_pca_outcome),\n",
    "            dp.Text(got.txt_kmeans_explore_cluster_count),\n",
    "            dp.Group(*plot_noiv_kmeans_list, columns=3),\n",
    "            dp.Group(\n",
    "                dp.Text(got.txt_kmeans_elbow),\n",
    "                dp.Plot(elb_visualizer.fig),\n",
    "                columns=2,\n",
    "            ),\n",
    "            dp.Text(got.txt_kmeans_silhouette),\n",
    "            dp.Group(*plot_noiv_kmeans_silhouette_list, columns=3),\n",
    "            dp.Text(got.txt_kmeans_outcome),\n",
    "            # dp.Text(got.txt_icdm),\n",
    "            # dp.Group(*plot_noiv_kmeans_icdm, columns=3),\n",
    "            dp.Text(got.txt_noiv_select_oiv),\n",
    "            dp.Group(*plot_noiv_kmeans_heat_map, columns=3),\n",
    "            dp.Text(got.txt_km_hm_conclusion),\n",
    "        ],\n",
    "    ),\n",
    "    dp.Page(\n",
    "        title=\"Conclusion\",\n",
    "        blocks=[\n",
    "            dp.Text(txt_noiv_conclusion),\n",
    "        ],\n",
    "    ),\n",
    "    layout=dp.PageLayout.TOP,\n",
    ").save(path=os.path.join(\".\", \"data_out\", \"reports\", \"mildiou-report.html\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheet by sheet predctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_exp = widgets.Dropdown(\n",
    "    options=list(df_inverted.experiment.sort_values(ascending=True).unique()),\n",
    "    desciption=\"Experiment\",\n",
    ")\n",
    "\n",
    "cb_sheet = widgets.Dropdown(options=[], description=\"Sheet\")\n",
    "\n",
    "op_plot = widgets.Output()\n",
    "\n",
    "\n",
    "def plot_prediction(df, exp, sheet):\n",
    "    X = df.drop([\"oiv\"], axis=1)\n",
    "    y = df.oiv\n",
    "    X = StandardScaler().fit(X).transform(X)\n",
    "    es_pls_da = PLSRegression(n_components=X.shape[1]).fit(X, y)\n",
    "    return gop.plot_model(\n",
    "        X=es_pls_da.x_scores_,\n",
    "        x_comp=1 - 1,\n",
    "        y_comp=2 - 1,\n",
    "        height=800,\n",
    "        color=y.astype(str),\n",
    "        title=f\"PLS-DA score for {exp} sheet {sheet}: {es_pls_da.score(X, y)}\",\n",
    "        axis_title_root=\"X-variate \",\n",
    "    )\n",
    "\n",
    "\n",
    "def predict_sheet(exp, sheet):\n",
    "    op_plot.clear_output()\n",
    "    with op_plot:\n",
    "        display(\n",
    "            plot_prediction(\n",
    "                (\n",
    "                    df_inverted[\n",
    "                        ((df_inverted.experiment == exp) & (df_inverted.sheet == sheet))\n",
    "                    ]\n",
    "                    .select_dtypes(exclude=object)\n",
    "                    .drop([\"colonne\"], axis=1)\n",
    "                    .drop_duplicates()\n",
    "                ),\n",
    "                exp,\n",
    "                sheet,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def on_experiment_change(change):\n",
    "    cb_sheet.options = list(\n",
    "        df_inverted[df_inverted.experiment == change.new].sheet.unique()\n",
    "    )\n",
    "    cb_sheet.index = 0\n",
    "    predict_sheet(change.new, cb_sheet.value)\n",
    "\n",
    "\n",
    "def on_sheet_change(change):\n",
    "    predict_sheet(cb_exp.value, change.new)\n",
    "\n",
    "\n",
    "cb_exp.observe(on_experiment_change, names=\"value\")\n",
    "cb_sheet.observe(on_sheet_change, names=\"value\")\n",
    "\n",
    "display(VBox([HBox([cb_exp, cb_sheet]), op_plot]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
