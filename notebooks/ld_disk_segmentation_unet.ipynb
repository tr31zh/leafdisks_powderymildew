{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf disk sheat segmentation and disk coordonates detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "import albumentations.augmentations.geometric as G\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ternausnet.models\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image as IpImage\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"scripts\"))\n",
    "\n",
    "import ld_dataset as ldd\n",
    "import ld_plot as ldp\n",
    "import ld_train_helpers as ldth\n",
    "import ld_image as ldi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_built() is True else \"cpu\"\n",
    "\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images = ldd.build_items_dataframe(\n",
    "    images_folder=ldd.train_images_folder,\n",
    "    masks_folder=ldd.train_masks_folder,\n",
    ")\n",
    "df_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train_images, test_size=0.3, stratify=df_train_images[\"year\"])\n",
    "test, val = train_test_split(test, test_size=0.5, stratify=test[\"year\"])\n",
    "\n",
    "print(len(train), len(test), len(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_image_grid(val.sample(n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb_resizer = A.Resize(height=256, width=256)\n",
    "\n",
    "train_transformers_list = [\n",
    "    alb_resizer,\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.3),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    A.CLAHE(p=0.36),\n",
    "]\n",
    "\n",
    "transformer = A.Compose(train_transformers_list)\n",
    "\n",
    "\n",
    "image, mask = ldd.open_image_and_mask(0, df_train_images)\n",
    "\n",
    "transformed = transformer(image=image, mask=mask)\n",
    "\n",
    "image_transformed = transformed[\"image\"]\n",
    "mask_transformed = transformed[\"mask\"]\n",
    "\n",
    "\n",
    "ldp.visualize_augmented_item(\n",
    "    image_transformed, mask_transformed, original_image=image, original_mask=mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_height, original_width, original_channel_count = image.shape\n",
    "\n",
    "(original_width, original_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = A.Compose(\n",
    "    train_transformers_list\n",
    "    + [\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_transformer = A.Compose(\n",
    "    [\n",
    "        alb_resizer,\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transformer = A.Compose(\n",
    "    [\n",
    "        alb_resizer,\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ldd.LeafDeafSegmentationDataset(\n",
    "    df_img=train,\n",
    "    transform=train_transformer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ldd.LeafDeafSegmentationDataset(\n",
    "    df_img=val,\n",
    "    transform=val_transformer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "ldp.visualize_augmented_dataset_item(train_dataset, idx=0, samples=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": \"UNet11\",\n",
    "    \"device\": device,\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 5,\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 30,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ldth.create_model(params)\n",
    "model = ldth.train_and_validate(model, train_dataset, val_dataset, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test.sample(n=10)\n",
    "\n",
    "test_dataset = ldd.LeafDeafSegmentationInferenceDataset(\n",
    "    image_list=test_sample.image_path.to_list(),\n",
    "    transform=test_transformer,\n",
    "    dataframe=test_sample,\n",
    ")\n",
    "\n",
    "\n",
    "predictions = ldth.test_model(model, params, test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = []\n",
    "for (predicted_256x256_mask, original_height, original_width) in predictions:\n",
    "    predicted_masks.append(\n",
    "        A.resize(\n",
    "            predicted_256x256_mask,\n",
    "            height=original_height,\n",
    "            width=original_width,\n",
    "            interpolation=cv2.INTER_NEAREST,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_image_grid(test_sample, predicted_masks=predicted_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict single annotated image image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train_images.sample(n=1)\n",
    "img_path = row.image_path.to_list()[0]\n",
    "src_image = ldd.open_image(img_path)\n",
    "\n",
    "predicted_mask = ldth.predict_image(\n",
    "    img_path,\n",
    "    model=model,\n",
    "    params=params,\n",
    "    threshold=0.5,\n",
    "    img_transformer=test_transformer,\n",
    ")\n",
    "\n",
    "ldp.visualize_item(src_image, predicted_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print mask as overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.visualize_image(cv2.bitwise_and(src_image, src_image, mask=predicted_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_grd_luma = 0.3\n",
    "lum, a, b = cv2.split(cv2.cvtColor(src_image, cv2.COLOR_BGR2LAB))\n",
    "lum = (lum * bck_grd_luma).astype(np.uint)\n",
    "lum[lum >= 255] = 255\n",
    "lum = lum.astype(np.uint8)\n",
    "background_img = cv2.merge((lum, a, b))\n",
    "background_img = cv2.cvtColor(background_img, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "ldp.visualize_image(\n",
    "    cv2.bitwise_and(background_img, background_img, mask=255 - predicted_mask)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.visualize_image(\n",
    "    cv2.bitwise_or(\n",
    "        cv2.bitwise_and(background_img, background_img, mask=255 - predicted_mask),\n",
    "        cv2.bitwise_and(src_image, src_image, mask=predicted_mask),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train_images.sample(n=1)\n",
    "img_path = row.image_path.to_list()[0]\n",
    "\n",
    "\n",
    "ldp.show_masked_image(\n",
    "    image=ldd.open_image(img_path),\n",
    "    mask=ldth.predict_image(\n",
    "        img_path,\n",
    "        model=model,\n",
    "        params=params,\n",
    "        threshold=0.5,\n",
    "        img_transformer=test_transformer,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction widgert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_image = widgets.Dropdown(\n",
    "    options=sorted([str(i) for i in ldd.train_images_folder.glob(\"*\")]),\n",
    "    description=\"Select an image:\",\n",
    ")\n",
    "\n",
    "sl_luma = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.0,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Background luma\",\n",
    ")\n",
    "sl_threshold = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.0,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description=\"Prediction threshold\",\n",
    ")\n",
    "\n",
    "output_image = widgets.Output()\n",
    "\n",
    "\n",
    "def on_prediction_widget_changed(image, threshold, luma):\n",
    "    output_image.clear_output()\n",
    "    with output_image:\n",
    "        ldp.show_masked_image(\n",
    "            image=ldd.open_image(image),\n",
    "            mask=ldth.predict_image(\n",
    "                image,\n",
    "                model=model,\n",
    "                params=params,\n",
    "                threshold=threshold,\n",
    "                img_transformer=test_transformer,\n",
    "            ),\n",
    "            luma=luma,\n",
    "        )\n",
    "\n",
    "\n",
    "def on_image_changed(change):\n",
    "    on_prediction_widget_changed(\n",
    "        image=change.new,\n",
    "        threshold=sl_threshold.value,\n",
    "        luma=sl_luma.value,\n",
    "    )\n",
    "\n",
    "\n",
    "def on_threshold_changed(change):\n",
    "    on_prediction_widget_changed(\n",
    "        image=dd_image.value,\n",
    "        threshold=change.new,\n",
    "        luma=sl_luma.value,\n",
    "    )\n",
    "\n",
    "\n",
    "def on_luma_changed(change):\n",
    "    on_prediction_widget_changed(\n",
    "        image=dd_image.value,\n",
    "        threshold=sl_threshold.value,\n",
    "        luma=change.new,\n",
    "    )\n",
    "\n",
    "\n",
    "dd_image.observe(on_image_changed, names=\"value\")\n",
    "sl_threshold.observe(on_threshold_changed, names=\"value\")\n",
    "sl_luma.observe(on_luma_changed, names=\"value\")\n",
    "\n",
    "display(\n",
    "    HBox([dd_image, sl_threshold, sl_luma]),\n",
    "    output_image,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf disk indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = dd_image.value\n",
    "\n",
    "\n",
    "mask=ldth.predict_image(\n",
    "    img_path,\n",
    "    model=model,\n",
    "    params=params,\n",
    "    threshold=0.5,\n",
    "    img_transformer=test_transformer,\n",
    ")\n",
    "\n",
    "ldp.visualize_item(ldd.open_image(img_path), mask, direction=\"re\", figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = ldi.index_contours(mask, threshold=0.8)\n",
    "\n",
    "ldp.visualize_image(\n",
    "    ldi.print_contours_indexs(\n",
    "        mask,\n",
    "        contours,\n",
    "        canvas=ldi.print_contour_threshold(mask, threshold=0.8),\n",
    "    ),\n",
    "    figsize=(12, 8),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = ldi.index_contours(mask, threshold=0.8)\n",
    "\n",
    "ldp.visualize_image(\n",
    "    ldi.print_contours_indexs(\n",
    "        mask,\n",
    "        contours,\n",
    "        canvas=ldd.open_image(img_path),\n",
    "    ),\n",
    "    figsize=(12, 8),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
