{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf disk sheat segmentation and disk coordonates detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ternausnet.models\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image as IpImage\n",
    "from IPython.display import display\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "import panel as pn\n",
    "import hvplot.pandas\n",
    "pn.extension()\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"scripts\"))\n",
    "\n",
    "import ld_dataset as ldd\n",
    "import ld_plot as ldp\n",
    "import ld_image as ldi\n",
    "import ld_th_pl_lightning as ldpl\n",
    "import gav_oidium_const as goc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build local images dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "una_images = [i.stem for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "ano_images = [i.stem for i in ldd.train_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "wot_images = [i.stem for i in ldd.wot_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ano_images), len(una_images), len(wot_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_images = list(set(ano_images + una_images + wot_images))\n",
    "len(local_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [\n",
    "    \"experiment\",\n",
    "    \"rep\",\n",
    "    \"image_name\",\n",
    "    \"ligne\",\n",
    "    \"colonne\",\n",
    "    \"oiv\",\n",
    "    \"sporulation\",\n",
    "    \"densite_sporulation\",\n",
    "    \"necrose\",\n",
    "    \"surface_necrosee\",\n",
    "    \"taille_necrose\",\n",
    "]\n",
    "\n",
    "df = (\n",
    "    pd.read_csv(os.path.join(\"..\", \"data_in\", \"raw_merged.csv\"), sep=\";\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.image_name.str.lower().replace(\"_\", \"-\").isin([l.lower().replace(\"_\", \"-\") for l in local_images])]\n",
    "\n",
    "df.sort_values(\"image_name\").to_csv(\n",
    "    os.path.join(\"..\", \"data_in\", \"local_raw_merged.csv\"),\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldpl.g_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check image duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "una_images = [str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "ano_images = [str(i) for i in ldd.train_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "\n",
    "len(una_images), len(ano_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check anotated images in not annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img for img in ano_images if img in una_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check not anotated images in annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img for img in una_images if img in ano_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images = ldd.build_items_dataframe(\n",
    "    images_folder=ldd.train_images_folder,\n",
    "    masks_folder=ldd.train_masks_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd.check_items_consistency(\n",
    "    images_folder=ldd.train_images_folder,\n",
    "    masks_folder=ldd.train_masks_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train_images, test_size=0.3, stratify=df_train_images[\"year\"])\n",
    "test, val = train_test_split(test, test_size=0.5, stratify=test[\"year\"])\n",
    "\n",
    "print(len(train), len(test), len(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set_counts = pd.DataFrame(\n",
    "    index=df_train_images.year.sort_values().value_counts(sort=False).index,\n",
    "    data={\n",
    "        \"train\": train.year.sort_values().value_counts(sort=False).values,\n",
    "        \"val\": val.year.sort_values().value_counts(sort=False).values,\n",
    "        \"test\": test.year.sort_values().value_counts(sort=False).values,\n",
    "    },\n",
    ")\n",
    "df_set_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set_counts.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(train.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(val.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldp.display_images_and_masks_grid(test.sample(n=2), fontsize=10, figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 32 * 21\n",
    "img_height = 32 * 14\n",
    "\n",
    "assert(img_width / img_height == 1.5)\n",
    "\n",
    "alb_resizer = [A.Resize(height=img_height, width=img_width)]\n",
    "\n",
    "train_transformers_list = alb_resizer + [\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.RandomGamma(p=0.33),\n",
    "    A.CLAHE(p=0.33),\n",
    "]\n",
    "\n",
    "transformer = A.Compose(train_transformers_list)\n",
    "\n",
    "\n",
    "image, mask = ldd.open_image_and_mask(\n",
    "    0, df_train_images.sample(n=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "transformed = transformer(image=image, mask=mask)\n",
    "\n",
    "to_tensor = [\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "train_transformer = A.Compose(train_transformers_list + to_tensor)\n",
    "\n",
    "val_transformer = A.Compose(alb_resizer + to_tensor)\n",
    "\n",
    "test_transformer = A.Compose(alb_resizer + to_tensor)\n",
    "\n",
    "\n",
    "ldp.visualize_augmentations(\n",
    "    image=image,\n",
    "    mask=mask,\n",
    "    augmentations=[transformer(image=image, mask=mask) for _ in range(5)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ldpl.LeafDiskSegmentation(\n",
    "    batch_size=16,\n",
    "    selected_device=ldpl.g_device,\n",
    "    learning_rate=0.0005,\n",
    "    max_epochs=400,\n",
    "    num_workers=0,\n",
    "    train_augmentations=train_transformer,\n",
    "    train_data=train,\n",
    "    val_augmentations=val_transformer,\n",
    "    val_data=val,\n",
    "    accumulate_grad_batches=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=model.max_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15, min_delta=0.0005),\n",
    "        DeviceStatsMonitor(),\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=3,\n",
    "            monitor=\"val_loss\",\n",
    "            auto_insert_metric_name=True,\n",
    "            filename=\"{epoch}-{step}-{train_loss:.5}-{val_loss:.5f}\",\n",
    "        ),\n",
    "    ],\n",
    "    accumulate_grad_batches=model.accumulate_grad_batches,\n",
    ")\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_overview = ldpl.update_overviews(test) \n",
    "version_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ldpl.LeafDiskPredictor()\n",
    "\n",
    "op_df_versions_overview = widgets.Output()\n",
    "with op_df_versions_overview:\n",
    "    display(\n",
    "        version_overview.drop([\"checkpoint_fileName\"], axis=1).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "cb_select_version = widgets.Dropdown(\n",
    "    options=[\"-1 | Select version\"]\n",
    "    + [\n",
    "        f\"{i} | {fn}\"\n",
    "        for i, fn in enumerate(version_overview.checkpoint_fileName.to_list())\n",
    "    ],\n",
    "    description=\"Select version\",\n",
    ")\n",
    "\n",
    "cb_image = widgets.Dropdown(\n",
    "    options=sorted([str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]),\n",
    "    description=\"Select an image:\",\n",
    ")\n",
    "\n",
    "cb_ancillary_image = widgets.Dropdown(\n",
    "    options=[\"Raw mask\", \"Cleaned mask\", \"Probabilities\"],\n",
    "    description=\"Ancillary image\",\n",
    ")\n",
    "\n",
    "src_image = widgets.Output()\n",
    "ancillary_image = widgets.Output()\n",
    "pred_image = widgets.Output()\n",
    "\n",
    "\n",
    "def update_images(image_path, anc_mode):\n",
    "    if predictor.model is None:\n",
    "        src_image.clear_output()\n",
    "        ancillary_image.clear_output()\n",
    "        pred_image.clear_output()\n",
    "        return\n",
    "\n",
    "    image = ldd.open_image(image_path)\n",
    "\n",
    "    predicted_mask = predictor.predict_image(image_path)\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    anc_img = (\n",
    "        predicted_mask\n",
    "        if anc_mode == \"Raw mask\"\n",
    "        else clean_mask\n",
    "        if anc_mode == \"Cleaned mask\"\n",
    "        else predictor.predict_image(image_path, return_probabilities=True)\n",
    "        if anc_mode == \"Probabilities\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    src_image.clear_output()\n",
    "    with src_image:\n",
    "        ldp.visualize_image(\n",
    "            image=image,\n",
    "            title=Path(image_path).stem,\n",
    "        )\n",
    "\n",
    "    ancillary_image.clear_output()\n",
    "    with ancillary_image:\n",
    "        plt.imshow(anc_img, cmap=plt.cm.RdPu)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    pred_image.clear_output()\n",
    "    with pred_image:\n",
    "        ldp.visualize_image(\n",
    "            ldi.print_contours_indexes(\n",
    "                clean_mask,\n",
    "                contours,\n",
    "                canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "            ),\n",
    "            figsize=(12, 8),\n",
    "        )\n",
    "\n",
    "\n",
    "def on_image_changed(change):\n",
    "    update_images(change.new, cb_ancillary_image.value)\n",
    "\n",
    "\n",
    "def on_anc_mode_changed(change):\n",
    "    update_images(cb_image.value, change.new)\n",
    "\n",
    "\n",
    "def on_version_changed(change):\n",
    "    idx, filename = str(change.new).replace(\" \", \"\").split(\"|\")\n",
    "    if int(idx) >= 0:\n",
    "        global predictor\n",
    "        predictor.model = ldpl.LeafDiskSegmentation.load_from_checkpoint(filename)\n",
    "        update_images(cb_image.value, cb_ancillary_image.value)\n",
    "    else:\n",
    "        predictor.model = None\n",
    "\n",
    "\n",
    "cb_select_version.observe(on_version_changed, names=\"value\")\n",
    "cb_image.observe(on_image_changed, names=\"value\")\n",
    "cb_ancillary_image.observe(on_anc_mode_changed, names=\"value\")\n",
    "\n",
    "display(\n",
    "    VBox(\n",
    "        [\n",
    "            HBox([op_df_versions_overview, cb_select_version]),\n",
    "            HBox([cb_image, cb_ancillary_image]),\n",
    "            HBox([VBox([src_image, ancillary_image]), pred_image]),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(cb_select_version.value).replace(\" \", \"\").split(\"|\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all available not used images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out_fld = ldd.root_folder.parent.parent.joinpath(\"data_out\", \"predictions\", datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
    "data_out_fld.mkdir(parents=True, exist_ok=True)\n",
    "data_out_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(ldd.una_images_folder.glob(\"*\")):\n",
    "    if i.stem.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    image = ldd.open_image(str(i))\n",
    "    predicted_mask = predictor.predict_image(str(i))\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    image = Image.fromarray(\n",
    "        ldi.print_contours_indexes(\n",
    "            clean_mask,\n",
    "            contours,\n",
    "            canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "        )\n",
    "    )\n",
    "    image.save(str(data_out_fld.joinpath(i.name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve isolated leaf disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_src_image = widgets.Dropdown(\n",
    "    options=sorted(\n",
    "        [str(i) for i in ldd.una_images_folder.glob(\"*\") if not i.stem.startswith(\".\")]\n",
    "    ),\n",
    "    description=\"Select a leaf dislk:\",\n",
    ")\n",
    "cb_row = widgets.Dropdown(options=[\"A\", \"B\", \"C\"])\n",
    "cb_col = widgets.Dropdown(options=[1, 2, 3, 4])\n",
    "\n",
    "op_full_plate = widgets.Output()\n",
    "op_leafdisk = widgets.Output()\n",
    "op_leafdisk_no_bck = widgets.Output()\n",
    "\n",
    "\n",
    "def show_leaf_disk(image_path, row, col):\n",
    "    image = ldd.open_image(image_path)\n",
    "\n",
    "    predicted_mask = predictor.predict_image(image_path)\n",
    "    clean_mask = ldi.clean_contours(mask=predicted_mask.copy(), size_thrshold=0.75)\n",
    "    contours = ldi.index_contours(clean_mask)\n",
    "\n",
    "    op_full_plate.clear_output()\n",
    "    op_leafdisk.clear_output()\n",
    "    op_leafdisk_no_bck.clear_output()\n",
    "\n",
    "    with op_full_plate:\n",
    "        ldp.visualize_image(\n",
    "            ldi.print_single_contour(\n",
    "                clean_mask,\n",
    "                contours,\n",
    "                row=row,\n",
    "                col=col,\n",
    "                canvas=ldi.apply_mask(image, clean_mask, draw_contours=8),\n",
    "            ),\n",
    "            figsize=(12, 8),\n",
    "        )\n",
    "\n",
    "    with op_leafdisk:\n",
    "        ldp.visualize_image(ldi.get_leaf_disk(image.copy(), contours, row, col))\n",
    "\n",
    "    with op_leafdisk_no_bck:\n",
    "        ldp.visualize_image(\n",
    "            ldi.get_leaf_disk(\n",
    "                image.copy(),\n",
    "                contours,\n",
    "                row,\n",
    "                col,\n",
    "                mask=clean_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def on_ld_image_changed(change):\n",
    "    show_leaf_disk(change.new, cb_row.value, cb_col.value)\n",
    "\n",
    "\n",
    "def on_ld_row_changed(change):\n",
    "    show_leaf_disk(cb_src_image.value, change.new, cb_col.value)\n",
    "\n",
    "\n",
    "def on_ld_col_changed(change):\n",
    "    show_leaf_disk(cb_src_image.value, cb_row.value, change.new)\n",
    "\n",
    "\n",
    "cb_src_image.observe(on_ld_col_changed, names=\"value\")\n",
    "cb_row.observe(on_ld_row_changed, names=\"value\")\n",
    "cb_col.observe(on_ld_col_changed, names=\"value\")\n",
    "\n",
    "display(\n",
    "    VBox(\n",
    "        [\n",
    "            HBox([cb_src_image, cb_row, cb_col]),\n",
    "            HBox([op_full_plate]),\n",
    "            HBox([op_leafdisk_no_bck, op_leafdisk]),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "57ea4112c076accd34380d1fc13840c87329161a9b2286676849023bcb84b091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
